---
title: "Tarea 1. Backpropagation"
output: html_notebook
---

## TAREA (los BONUS 1 y 2 son opcionales):
### David Montero Loaiza

Generalizar la función backprop anterior para que contemple la inclusión de una capa oculta. Aplicar la función al ejemplo de la clasificación circular. Si se fija el número máximo de épocas en 1000. ¿Qué número de neuronas
ocultas y qué valor de la tasa de aprendizaje (eta) es óptimo para este problema? (basta una solución aproximada).

*BONUS 1* - ¿Sabrías incluir un término de inercia en el método de backpropagation (ver transparencia 4)?
*BONUS 2* - Generalizar a un número arbitrario de capas (recomendación, definir los pesos como una lista de matrices, una para cada capa).

```{r}
backprop_mlp = function(X,y,h = c(3,2),epochs = 10,eta = 0.1){
  
  print("Iniciando ajuste...")
  print("...")
  
  # AGREGAMOS EL BIAS
  # ES DECIR, AGREGAMOS UNA COLUMNA DE 1'S
  X = cbind(X,rep(1,nrow(X)))
  print("Tamaño de X con bias:")
  print(dim(X))
  print("**************************************************")
  
  # CANTIDAD DE NEURONAS POR CAPA
  neurons = c(ncol(X),h,ncol(y))
  print("Cantidad de neuronas por capa:")
  print(neurons)
  print("**************************************************")
  
  # PESOS INICIALES ALEATORIOS
  n_weights = length(neurons) - 1
  
  # INICIALIZAMOS UNA LISTA DE PESOS VACIA
  W = list()
  
  # PARA CADA LISTA DE PESOS CREAMOS UNA MATRIZ DE PESOS ALEATORIA
  for(i in 1:n_weights){
    
    W[[i]] = matrix(data = runif(neurons[i]*neurons[i + 1],min = -1, max = 1),
                    nrow = neurons[i + 1],
                    ncol = neurons[i])
    
  }
  
  print("Pesos inicializados...")
  print("...")
  
  # FUNCION DE ACTIVACION SIGMOIDE Y SU DERIVADA
  activation = function(x) 1/(1 + exp(-x))
  derivative = function(x) x*(1 - x)
  
  # VECTOR VACIO PARA GUARDAR ERRORES
  MSE = NULL
  
  print("Comenzando iteraciones, este proceso puede tardar...")
  print("...")
  
  # COMENZANDO ITERACIONES
  for(epoch in 1:epochs){
    
    # LISTAS VACIAS PARA GUARDAR
    # 1. PRODUCTOS WX
    # 2. ACTIVACION f(WX)
    products = list()
    activated = list()
    
    # GENERAR LOS PRODUCTOS
    for(i in 1:n_weights){
      
      # CALCULO DE PRODUCTOS
      # SI ES EL PRIMER CALCULO DE PRODUCTOS EL RESULTADO ES XW
      # SI NO LO ES EL RESULTADO ES LA ACTIVACION ANTERIOR POR W
      if(i == 1){products[[i]] = X %*% t(W[[i]])}else{products[[i]] = activated[[i - 1]] %*% t(W[[i]])}
      
      # CALCULO DE ACTIVACIONES
      activated[[i]] = activation(products[[i]])
      
    }
    
    # CALCULO DEL ERROR
    error = y - activated[[n_weights]]
    
    # CALCULO DE DELTA
    # ERROR * (ERROR * (1 - ERROR))
    delta = error * derivative(error)
    
    # LISTA VACIA PARA CALCULAR LOS DELTAS DEL GRADIENTE
    gd = list()
    
    # SE CALCULAN LOS DELTAS DEL GRADIENTE
    for(i in n_weights:1){
      
      gd[[i]] = t(delta)
      
      for(j in n_weights:i){
        
        if(j == i){
          
          if((j - 1) > 0){
            
            gd[[i]] = gd[[i]] %*% activated[[j-1]]
            
          }else{
            
            gd[[i]] = gd[[i]] %*% X
            
          }
          
        }else{
          
          gd[[i]] = t(t(gd[[i]]) %*% W[[j]] * derivative(products[[j - 1]]))
        }
        
      }
      
    }
    
    # SE CALCULA Y SE GUARDA EL ERROR POR EPOCA
    MSE = c(MSE,sum(error^2)/nrow(error))
    
    # SE ACUALIZAN LOS PESOS
    for(i in 1:n_weights){
      
      W[[i]] = W[[i]] - eta * gd[[i]]
      
    }
    
  }
  
  # SE CREA UNA LISTA CON LAS VARIABLES A RETORNAR
  to_return = list(error = MSE,weights = W,values = activated[[n_weights]])
  
  print("Ajuste finalizado")
  
  return(to_return)
  
}
```


```{r}
lin = read.csv("circle.csv",header = FALSE)

ind = which(lin[,3] == 0)

plot(lin[ind,1],lin[ind,2],type="p",xlim=c(-1,1),ylim=c(-1,1))
lines(lin[-ind,1],lin[-ind,2],type="p",col="red")
```

```{r}
a = as.matrix(lin[,-3])
dim(a)
```

```{r}
b = as.matrix(lin[,3])
dim(b)
```

```{r}
ajuste = backprop_mlp(a,b,h = c(5,3,3),epochs = 1000,eta = 0.01)
```

```{r}
plot(ajuste$error)
```

