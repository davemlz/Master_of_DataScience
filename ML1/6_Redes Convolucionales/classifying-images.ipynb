{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "classifying-images.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwiQhtfSm28t",
        "colab_type": "text"
      },
      "source": [
        "# David Montero Loaiza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucWv1yC-lSua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "689405cd-4331-4913-e458-39dc67d618d5"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYZu48nMlSul",
        "colab_type": "text"
      },
      "source": [
        "# Using convnets with small datasets\n",
        "\n",
        "The first thing to do is download the following file https://lara.web.cern.ch/lara/train.zip in the jupyter terminal and uncompress it in the same folder as this notebook. \n",
        "\n",
        "To download another dataset from imagenet you can do it with the URL list of the images and using `wget -i`\n",
        "\n",
        "\n",
        "\n",
        "## Training a convnet from scratch\n",
        "\n",
        "Training an image classification model with very little data is a common situation you will find yourself in if you end up doing Computer Vision in a professional context.  \n",
        "\n",
        "Having \"few\" samples can mean anything from a few hundred to a few tens of thousands of images.  Let's illustrate a practical example here: let's focus on classifying images as \"dogs\" and \"cats\". \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5EvYBqHlSun",
        "colab_type": "text"
      },
      "source": [
        "## The importance of DEEP Learning in problems with few data\n",
        "\n",
        "You may have heard many times that Deep Learning only works when you have large amounts of data. This is partly true: one of the characteristics of Deep learning is that you can find interesting features from the training dataset itself, and this a priori is easier when you have many examples available, especially in the case of having input datasets with a high dimensionality, such as images.\n",
        "\n",
        "However, what constitutes a \"large\" dataset is relative. Specifically relative to the size and depth of the network we are trying to train. It is not possible to sand a convnet so that it becomes a complete problem with only a few dozen examples, but a few hundred can be enough if the model is well assembled (we will understand what \"well assembled\" means throughout the Deep Learning course).\n",
        "\n",
        "Since convnets learn local characteristics, invariant under translations, they are very efficient in terms of the number of images needed to carry out perceptual problems. So training a convnet from 0 with a not very large dataset can still lead to reasonable results as we will see here.\n",
        "\n",
        "But there is more: Deep Learning models are highly \"recyclable\". One can, for example, take an image classification problem and a trained speech-to-text converter on a very big dataset and then reuse it for solving a completely different problem only by adding some small modifications. More specifically, in the case of Computer Vision, many pre-trained models (usually trained on the ImageNet dataset) are made public so that one can download them and use them to create powerful Computer Vision models with very little data.\n",
        "\n",
        "But here we will just run a simple example.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksmICrgXlSuo",
        "colab_type": "text"
      },
      "source": [
        "## Los datos\n",
        "\n",
        "The cat vs dog dataset we use is not a Keras package. It was posted on Kaggle.com as part of a Computer Vision problem in late 2013, when ConvNets were not yet so popular. \n",
        "\n",
        "The images are medium resolution JPGEs. It looks like this:\n",
        "\n",
        "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzWK9CjHlSut",
        "colab_type": "text"
      },
      "source": [
        "It's no surprise that the 2013 Kaggle cat vs dog competition was won by ConvNets. The best were able to achieve up to 95% accuracy. In our example we are still far from this accuracy, but during the Deep Learning course we have learned how to approach this value using different methods to improve the performance of neural networks. It should be noted that in this example we are training on approximately only 10% of the data that was used for the contest. \n",
        "After downloading the dataset and decompressing it, we are going to create a new dataset containing three subsets: a training set containing 1000 images of each class, a validation set with 500 images of each class, and finally a test set with 500 images of each class.\n",
        "\n",
        "Here we have a few lines of code that make us this distribution automatically:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFf6Iy0Ynaz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/train\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "garUp9-wlSuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL_MlwrFlSu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The path to the directory where the original\n",
        "# dataset was uncompressed\n",
        "original_dataset_dir = '/content/drive/My Drive/train'\n",
        "\n",
        "# The directory where we will\n",
        "# store our smaller dataset\n",
        "base_dir = '/content/drive/My Drive/cats_and_dogs_small'\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "os.mkdir(train_cats_dir)\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "os.mkdir(validation_cats_dir)\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "os.mkdir(validation_dogs_dir)\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "os.mkdir(test_cats_dir)\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "# Copy first 1000 cat images to train_cats_dir\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "# Copy next 500 cat images to validation_cats_dir\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy next 500 cat images to test_cats_dir\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy first 1000 dog images to train_dogs_dir\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy next 500 dog images to validation_dogs_dir\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# Copy next 500 dog images to test_dogs_dir\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVnEHCS0lSu9",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOq5uKJ1lSu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee6431bb-fee8-49d2-8637-fa696d67babe"
      },
      "source": [
        "print('total training cat images:', len(os.listdir(train_cats_dir)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training cat images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7vs8qo0lSvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b94abac-b57c-47d5-fd5b-31c387e8c80c"
      },
      "source": [
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training dog images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTa9GcFXlSvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e21ae64e-2751-449a-d014-c09bce33a25e"
      },
      "source": [
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total validation cat images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl8GRyEelSvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9320dc3-0581-47e1-ee84-2abe56439451"
      },
      "source": [
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total validation dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruHxq66HlSvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f79ff05-3340-4ae9-f552-c14697e554bc"
      },
      "source": [
        "print('total test cat images:', len(os.listdir(test_cats_dir)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test cat images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdDoJpLTlSvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b99dee6e-42ac-409e-d0e0-2b5601f6dacd"
      },
      "source": [
        "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF8R-x2HlSvr",
        "colab_type": "text"
      },
      "source": [
        "So effectively we have 2000 training images, 1000 validation images and 1000 test images. In each of these subsets there are the same number of examples from each class: this is what is called a balanced binary classification system, which means that our classification accuracy will be an adequate metric of the success of our solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVI-hnCslSvs",
        "colab_type": "text"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "In the above example we have built a small convnet to solve the problem of classifying handwritten numbers using the MNIST dataset, so we are already familiar with the terminology that keras uses. We are going to reuse the general structure we had in the previous example: our convnet will have a stack of alternate layers of `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
        "\n",
        "However, since we are dealing with larger images and a more complex problem, we will create our network accordingly: it will have one more layer of `Conv2D` + `MaxPooling2D`. This serves to increase the capacity of the network and to further reduce the size of the feature maps, so that they are not so huge when they reach the flattening step. We start using 150x150 input images (an arbitrary choice), and end up with feature maps that are 7x7 in size before the flattening layer.\n",
        "\n",
        "It is important to note that the depth of feature maps progressively increases as we move through the neural network (from 32 to 128) while the size of feature maps decreases (from 148x148 to 7x7). You will see this pattern in almost all convnets.\n",
        "\n",
        "As we are attacking a binary classification problem (dog or cat), we are going to finish the network with a single unit (a dense layer of size 1) and with a sigmoid activation. This unit will encode the probability that our network is looking at one class or another.\n",
        "\n",
        "The final look of the model should be as follows:\n",
        "\n",
        "\n",
        "![modelo_red_animales.png](https://github.com/laramaktub/MachineLearningI/blob/master/modelo_red_animales.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pINGPnT8lSvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "4232c3e6-3fce-42e2-8dab-80f78a31fbd0"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32,(3,3), activation='relu', input_shape = (150,150,3)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(512,activation=\"relu\"))\n",
        "model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjYhubTRlSvw",
        "colab_type": "text"
      },
      "source": [
        "Para el paso de compilación utilizaremos el optimizador `RMSprop`(lr=1e-4). Como nuestra red termina con una única unidad sigmoide, vamos a utilizar binary crossentropy como nuestra función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaUfMI6ulSvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr = 1e-4),loss=\"binary_crossentropy\",metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jpA49XIlSv1",
        "colab_type": "text"
      },
      "source": [
        "##  Data preprocessing\n",
        "\n",
        "The images must be properly formatted as float tensors before they are given to the net. That's just what we're going to do here. Before we pre-process them, the images are JPEG files. The steps to be able to give them to our network are roughly as follows:\n",
        "\n",
        "* Read the files with the images.\n",
        "* Decode the content of the JPEG in a \"grid\" with the RGB of the pixels \n",
        "* Turn that \"grill\" into floatation devices\n",
        "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval as neural networks prefer to work with small values. \n",
        "\n",
        "All this may seem very complicated but thanks to Keras our life is much easier and we can count on your tools to take care of these steps automatically. Keras has a module with tools for image processing, which can be found in `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` that allows us to automatically convert images we have on the hard disk into pre-processed tensors. This is exactly what we'll be using next. To do this we can use the flow_from_directory to take the images directly from the folders that we previously generated. We give it as input the folders where the training (or validation) images are, the size of the images (target_size), the size of the batch we're going to use (we're going to start with 20) and as there are only two categories, we tell it that we're going to use binary_crossentropy (class_mode). When we run these commands we'll get the following, the total number of images and how many classes there are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfPjwDqklSv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f72e8e53-a5ac-4d08-fdff-75af5582ad09"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,target_size=(150, 150),batch_size=20,class_mode='binary')\n",
        "validation_generator = validation_datagen.flow_from_directory(validation_dir,target_size=(150, 150),batch_size=20,class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,target_size=(150, 150),batch_size=20,class_mode='binary')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdOnyL0wlSv5",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at one of these generators: it takes us to a batch of 150x150 RGB images (dimensions `(20, 150, 150, 3)`) and binary tags (dimension `(20,)`). 20 is the number of examples in each batch (what we call the batch size). The generator generates these batches indefinitely: it runs a loop endlessly through all the images we have in the folder. That's why we have to type `break` to break the loop at some point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAwjtFzqlSv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "65675d7f-e278-4f01-bf78-941aead8592f"
      },
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data batch shape: (20, 150, 150, 3)\n",
            "labels batch shape: (20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9F_sGolSv_",
        "colab_type": "text"
      },
      "source": [
        "Now let's make the fit. In this case, as what we have is a generator, we use fit_generator. We are going to run 30 epochs and use the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2P7puoZlSwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d60c83b-633f-4362-c584-26afb0acf704"
      },
      "source": [
        "history = model.fit_generator(generator=train_generator,epochs=30,validation_data=validation_generator)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.6899 - acc: 0.5325 - val_loss: 0.6687 - val_acc: 0.6090\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.6526 - acc: 0.6195 - val_loss: 0.6601 - val_acc: 0.5780\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.6032 - acc: 0.6690 - val_loss: 0.7120 - val_acc: 0.5630\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.5647 - acc: 0.6975 - val_loss: 0.5897 - val_acc: 0.6950\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.5268 - acc: 0.7300 - val_loss: 0.5864 - val_acc: 0.6790\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.4975 - acc: 0.7570 - val_loss: 0.6251 - val_acc: 0.6610\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.4710 - acc: 0.7750 - val_loss: 0.5752 - val_acc: 0.7020\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.4352 - acc: 0.7905 - val_loss: 0.5693 - val_acc: 0.7010\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.4031 - acc: 0.8220 - val_loss: 0.5485 - val_acc: 0.7120\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.3815 - acc: 0.8315 - val_loss: 0.5648 - val_acc: 0.7180\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3428 - acc: 0.8505 - val_loss: 0.6850 - val_acc: 0.6960\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.3168 - acc: 0.8630 - val_loss: 0.6211 - val_acc: 0.7080\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.2968 - acc: 0.8825 - val_loss: 0.5733 - val_acc: 0.7390\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.2712 - acc: 0.8905 - val_loss: 0.5612 - val_acc: 0.7510\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.2422 - acc: 0.9050 - val_loss: 0.6039 - val_acc: 0.7270\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.2183 - acc: 0.9145 - val_loss: 0.6065 - val_acc: 0.7400\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.1969 - acc: 0.9280 - val_loss: 0.6184 - val_acc: 0.7340\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.1695 - acc: 0.9350 - val_loss: 0.7111 - val_acc: 0.7170\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.1453 - acc: 0.9500 - val_loss: 0.8948 - val_acc: 0.6910\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.1323 - acc: 0.9605 - val_loss: 0.9059 - val_acc: 0.7120\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.1191 - acc: 0.9545 - val_loss: 0.7426 - val_acc: 0.7340\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.0992 - acc: 0.9700 - val_loss: 0.8446 - val_acc: 0.7290\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.0865 - acc: 0.9705 - val_loss: 0.8005 - val_acc: 0.7350\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.0834 - acc: 0.9735 - val_loss: 0.8129 - val_acc: 0.7300\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.0619 - acc: 0.9850 - val_loss: 0.8833 - val_acc: 0.7440\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.0523 - acc: 0.9850 - val_loss: 0.9208 - val_acc: 0.7380\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.0515 - acc: 0.9865 - val_loss: 0.9970 - val_acc: 0.7330\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.0409 - acc: 0.9865 - val_loss: 1.0309 - val_acc: 0.7290\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.0347 - acc: 0.9910 - val_loss: 1.1399 - val_acc: 0.7300\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.0279 - acc: 0.9910 - val_loss: 1.0547 - val_acc: 0.7300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "uNjIAWaclSwF",
        "colab_type": "text"
      },
      "source": [
        "It's a nice idea to save the model after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXDNFzxYlSwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/model/cat_dog.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX5Y02vUlSwI",
        "colab_type": "text"
      },
      "source": [
        "Let's now evaluate our model using the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIhsvMMElSwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate_generator(test_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDxSvl4R_mcN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd9e1c6b-7b42-44c4-b2db-fdbcfc1b5c8c"
      },
      "source": [
        "test_loss"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.078596914112568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZbPXqZB_oBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c10a9b38-a4bc-4416-c3b9-1dc06793ae5e"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7329999995231629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKGKI-aZlSwN",
        "colab_type": "text"
      },
      "source": [
        "Try to optimize the network with the tools you have learnt during the lesson. Try to make improvements both in terms of speed and accuracy. Comment the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htPJD6HelSwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "903a303f-9561-4b43-ad0a-d4bd80466d1a"
      },
      "source": [
        "new_model = models.Sequential()\n",
        "\n",
        "new_model.add(layers.Conv2D(32,(3,3), activation='relu', input_shape = (150,150,3)))\n",
        "new_model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
        "new_model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(layers.Conv2D(128,(3,3), activation='relu'))\n",
        "new_model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(layers.Conv2D(128,(3,3), activation='relu'))\n",
        "new_model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "new_model.add(layers.Flatten())\n",
        "\n",
        "new_model.add(layers.Dropout(0.5))\n",
        "\n",
        "new_model.add(layers.Dense(512,activation=\"relu\"))\n",
        "new_model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_29 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AxO2wPkE_nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model.compile(optimizer=optimizers.RMSprop(lr = 1e-4),loss=\"binary_crossentropy\",metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA6qKaZgFILQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "a486cf45-9bbe-4c72-f858-91218241d52c"
      },
      "source": [
        "from keras import callbacks\n",
        "\n",
        "callback_es = callbacks.EarlyStopping(patience=8)\n",
        "\n",
        "history = new_model.fit_generator(generator=train_generator,epochs=30,validation_data=validation_generator,callbacks = [callback_es])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.6939 - acc: 0.5220 - val_loss: 0.6790 - val_acc: 0.5830\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 108s 1s/step - loss: 0.6725 - acc: 0.5825 - val_loss: 0.6527 - val_acc: 0.6280\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 108s 1s/step - loss: 0.6366 - acc: 0.6380 - val_loss: 0.6320 - val_acc: 0.6300\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.5968 - acc: 0.6830 - val_loss: 0.6472 - val_acc: 0.6370\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.5735 - acc: 0.7030 - val_loss: 0.6253 - val_acc: 0.6540\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.5587 - acc: 0.7105 - val_loss: 0.5775 - val_acc: 0.6970\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.5280 - acc: 0.7330 - val_loss: 0.5911 - val_acc: 0.6780\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.5051 - acc: 0.7460 - val_loss: 0.5732 - val_acc: 0.6900\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.4845 - acc: 0.7695 - val_loss: 0.5792 - val_acc: 0.6890\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.4587 - acc: 0.7860 - val_loss: 0.5594 - val_acc: 0.7020\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.4447 - acc: 0.7855 - val_loss: 0.5509 - val_acc: 0.7130\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.4316 - acc: 0.7930 - val_loss: 0.5657 - val_acc: 0.7190\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.4118 - acc: 0.8125 - val_loss: 0.5712 - val_acc: 0.7160\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.3843 - acc: 0.8305 - val_loss: 0.7170 - val_acc: 0.6660\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3822 - acc: 0.8260 - val_loss: 0.5458 - val_acc: 0.7330\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3594 - acc: 0.8435 - val_loss: 0.5697 - val_acc: 0.7220\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3471 - acc: 0.8490 - val_loss: 0.5930 - val_acc: 0.7260\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3339 - acc: 0.8545 - val_loss: 0.5841 - val_acc: 0.7270\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3226 - acc: 0.8655 - val_loss: 0.5987 - val_acc: 0.7300\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3162 - acc: 0.8600 - val_loss: 0.5948 - val_acc: 0.7290\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.3102 - acc: 0.8680 - val_loss: 0.6137 - val_acc: 0.7320\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.2834 - acc: 0.8770 - val_loss: 0.7273 - val_acc: 0.7100\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.2694 - acc: 0.8900 - val_loss: 0.6206 - val_acc: 0.7350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh6bzm3ta8Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model.save('/content/drive/My Drive/model/cat_dog_callback_dropout.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djdiKGLlbIaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_test_loss, new_test_acc = new_model.evaluate_generator(test_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJLIcT72bNrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "903791a8-ca98-45da-a947-c0a72aec7abd"
      },
      "source": [
        "new_test_loss"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6031138455867767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1GjbdThbN9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cee992cf-c37d-4ca3-a86d-0c5d10f05736"
      },
      "source": [
        "new_test_acc"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7489999997615814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}