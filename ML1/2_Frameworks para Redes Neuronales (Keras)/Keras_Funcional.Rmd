---
title: "R Notebook"
output: html_notebook
---

Práctica KERAS en R con el dataset MNIST
1. Instalar KERAS
KERAS es una librería para la construcción de redes neuronales muy versátil y ampliamente utilizada por la comunidad de deep learning. Aunque fue diseñada originalmente para python en los últimos años se ha desarrollado una interfaz de KERAS en R para que sus usuarios también puedan beneficiarse de la potencia de KERAS. Información sobre por qué usar KERAS como librería principal para el diseño de redes neuronales puede encontrarse en el siguinte link: https://keras.rstudio.com/articles/why_use_keras.html . En cuanto a la instalación hay que hacer lo siguiente:

Ir a la terminal y teclear: pip install keras

```{r}
# install.packages("keras")
# library(keras)
# install_keras()
```

```{r}
require(keras)
```

2. Construccion de un modelo de KERAS
Hay dos maneras de construir un modelo en KERAS: el modelo secuencial y el modelo funcional. En el primer caso la red es una secuencia lineal de capas, mientras que el segundo caso permite una elaboración de redes con topologías más complejas (como por ejemplo redes acícliclas). Los pasos para construir una red son:

1- Diseñar la arquitectura de la red (ya sea mediante la manera secuencial o funcional).

2- Compilar el modelo (aqui se incluye la seleccion del algoritmo de aprendizaje y la funcion de coste a minimizar).

3- En este paso se introduce el dataset, la separación entre validacion y train, nº de epocas. En resumen se entrena la arquiterctura del paso 1 con el algoritmo seleccionado en el paso 2.

4- Utilizar el modelo para predecir.

2.1 El modelo secuencial

```{r}
# model <- keras_model_sequential() 
# model %>% 
#   layer_dense(units = neuronas ocultas 1, input_shape = neuronas de entrada, activation = "sigmoid") %>% # Primera capa oculta
#   layer_dense(units = neuronas ocultas 2, activation = "sigmoid") %>% # Segunda capa oculta
#   layer_dense(units = neuronas de salida , activation = "sigmoid") # capa de salida
```

2.2 El modelo funcional

```{r}
# inputs = layer_input(shape = dim(xT)[-1])
# x = inputs
# l1 = layer_dense(x,units = neuronas ocultas 1, activation = "sigmoid")
# l2 = layer_dense(l1,units = neuronas ocultas 2, activation = "sigmoid")
# outputs = layer_dense(l1,units = neuronas de salida, activation = "sigmoid")
# model <- keras_model(inputs = inputs, outputs = outputs)
```

2.3 Compilar el modelo y entrenar

```{r}
#  model %>% compile(optimizer = optimizer_adam(lr = learning_rate), 
#                     loss = "mse")
#  model %>% fit(xT, yT, epochs = epochs, batch_size = 100, validation_split = 0.1, callbacks = callbacks, verbose = 1)  
```

2.4 Predecir

```{r}
# prediction <- model$predict(xt) 
```

3. Construir una red neuronal para el dataset MNIST
3.1 Cargar el dataset MNIST

```{r}
# data = read.csv("MNIST_train.csv")
mnist = dataset_mnist()
x_train = mnist$train$x
y_train = mnist$train$y
x_test = mnist$test$x
y_test = mnist$test$y
```

```{r}
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
y_train <- to_categorical(y_train, 10)
```


3.2 Preprocesar los datos

```{r}
# normalización, adecuar las matrices con las columnas apropiadas (por ejemplo, 784 en la entrada) ...
## matriz de salida (neuronas de salida)
## matriz de entrada (neuronas de entrada)
## normalizar/escalar
## train/test (los primeros 30000 son train y el resto test)
```

3.3.1 Construir una red con estructura 784-100-100-10 mediante el modelo secuencial y entrenar

```{r}
# ESCRIBE AQUI
```

3.3.2 Construir una red con estructura 784-100-100-10 mediante el modelo funcional y entrenar

```{r}
inputs = layer_input(shape = 784)
l1 = layer_dense(inputs,units = 100, activation = "sigmoid")
l2 = layer_dense(l1,units = 100, activation = "sigmoid")
outputs = layer_dense(l2,units = 10, activation = "sigmoid")
model = keras_model(inputs = inputs, outputs = outputs)
```

```{r}
model %>% compile(optimizer = optimizer_adam(lr = 0.1),loss = "mse")
model %>% fit(x_train,y_train,epochs = 10, batch_size = 100, validation_split = 0.1, verbose = 1) 
```

3.4 Predecir en el conjunto de test y validar para el modelo funcional

```{r}
pred = predict(model,x_test)
# pred = predict_classes(model,x_test)
```

Ahora valida calculando el AUC por dígito y el accuracy tal y como visteis en la práctica de KNN

```{r}
# install.packages("verification") para importar la funcion roc.area()
## AUC
## Accuracy
```

3.5 Reponde a las siguientes preguntas
1 - ¿Para qué número obtengo el AUC más alto? ¿y para cuál el más bajo?

```{r}
# ESCRIBE AQUÍ
```

2 - Construir una red con estructura 784-50-25-50-10, ¿qué es lo que sucede en el entrenamiento?¿por qué no funciona bien?

```{r}
# ESCRIBE AQUÍ
```

3- ¿Crees que para problemas de clasificación es apropiado minimizar el mean squared error? Construye una red minimizando el categorical cross-entropy, tal que en compile(loss = "categorical_crossentropy") y vuelve a calcular los índices de validación...¿ves alguna diferencia con respecto a cuando se entrenó minimizando el "mse"?

```{r}
# ENTRENA AQUÍ CON CATEGORICAL CROSSENTROPY
# PREDICE Y VALIDA AQUI
```

3.6 Early-Stopping y como guardar el modelo
En las redes neuronales hay muchos parámetros y por tanto en muchas ocasiones es necesario adoptar medidas de regularización como el early-stopping. El early-stopping consiste en parar el entrenamiendo cuando se cumpla un criterio como los siguientes:

1- Que la diferencia del error entre el dataset de train y el dataset de validación no supere un mínimo

2- Cuando el error de validación deje de disminuir.
La manera de implementarlo en KERAS es a través de los callbacks. Los callbacks se refieren a funciones que solo aplican durante el entrenamiento (por ejemplo, el early-stopping solo tiene sentido mientras el modelo entrena, cuando ya está entrenado no hay nada que parar). El registro completo de callbacks definidos en KERAS puede verse en el siguiente link: https://keras.io/callbacks/ . En esta clase vamos a ver dos funciones pertenecientes a los callbacks: el callback del early-stopping y el callback de guardar el modelo en cada época.

3.6.1 Callback del early-stopping

```{r}
patience = 10

callbacks = list(callback_early_stopping(patience = patience))

inputs = layer_input(shape = dim(x_train)[-1])
x = inputs
l1 = layer_dense(x,units = neuronas ocultas 1, activation = "sigmoid")
l2 = layer_dense(l1,units = neuronas ocultas 2, activation = "sigmoid")
outputs = layer_dense(l2,units = neuronas de salida, activation = "sigmoid")

model <- keras_model(inputs = inputs, outputs = outputs)
model %>% compile(optimizer = optimizer_sgd(lr = learning_rate),loss = "mse")

model %>% fit(x_train,y_train,epochs = epochs, batch_size = 100, validation_split = 0.1, callbacks = callbacks, verbose = 1)
```

3.6.2 Callback de guardar el modelo

```{r}
# callbacks = list(callback_model_checkpoint(filepath='filename.h5'))
# inputs = layer_input(shape = dim(xT)[-1])
# x = inputs
# l1 = layer_dense(x,units = neuronas ocultas 1, activation = "sigmoid")
# l2 = layer_dense(l1,units = neuronas ocultas 2, activation = "sigmoid")
# outputs = layer_dense(l2,units = neuronas de salida, activation = "sigmoid")
# model <- keras_model(inputs = inputs, outputs = outputs)
#  model %>% compile(optimizer = optimizer_sgd(lr = learning_rate), 
#                     loss = "mse") 
#  model %>% fit(xT, yT, epochs = epochs, batch_size = 100, validation_split = 0.1, callbacks = callbacks, verbose = 1) 
```

3.6.3 Guardar únicamente el mejor modelo de acuerdo al criterio del early-stopping... y cargarlo
```{r}
patience = 10
learning_rate = 0.1
epochs = 10000

callbacks = list(callback_early_stopping(patience = patience),
                 callback_model_checkpoint(filepath=paste0('filename.h5'), monitor='val_loss', save_best_only=TRUE))

inputs = layer_input(shape = 784)
x = inputs
l1 = layer_dense(x,units = 100, activation = "sigmoid")
l2 = layer_dense(l1,units = 100, activation = "sigmoid")
outputs = layer_dense(l2,units = 10, activation = "sigmoid")

model <- keras_model(inputs = inputs, outputs = outputs)
model %>% compile(optimizer = optimizer_sgd(lr = learning_rate),loss = "mse")
model %>% fit(x_train,y_train, epochs = epochs, batch_size = 100, validation_split = 0.1, callbacks = callbacks, verbose = 1)
k_clear_session()
load_model_hdf5(filepath = "filename.h5")
```

3.6.4 Responde a las siguientes preguntas
Entrena el mismo modelo con y sin early-stopping (sin early-stopping quiere decir que entreneis 200 épocas, por ejemplo). Cargalo y calcula el AUC.

1- ¿qué modelo obtiene un AUC mayor para todos los números, el del early-stopping o el que no, en el dataset de TRAIN?

2- ¿qué modelo obtiene un AUC mayor para todos los números, el del early-stopping o el que no, en el dataset de TEST?
```{r}
# ENTRENA AQUI SIN EARLY-STOPPING
# ENTRENA AQUI CON EARLY-STOPPING
## PREDICCION EN EL TRAIN CON LOS DOS MODELOS (RECORDAD QUE EL DE EARLY-STOPPING HAY QUE CARGARLO)
## AUC
## Accuracy
## PREDICCION EN EL TEST CON LOS DOS MODELOS (RECORDAD QUE EL DE EARLY-STOPPING HAY QUE CARGARLO)
## AUC
## Accuracy
```

4. Diseña una arquitectura original
Un ejemplo de arquitectura atípica serian las deep residual networks: 
Image

```{r}
inputs = layer_input(shape = 784)

h1 = layer_dense(inputs,units = 20, activation = "sigmoid")

h2 = layer_dense(h1,units = 10, activation = "sigmoid")

h3a = layer_dense(h2,units = 20)
h3b = layer_dense(h1,units = 20)

h3_no_activada = h3a + h3b

h3 = layer_activation(h3_no_activada,activation = "sigmoid")

oa = layer_dense(inputs,units = 784)
ob = layer_dense(h3,units = 784)

o_sin_activar = oa + ob

outputs = layer_activation(o_sin_activar,activation = "sigmoid")

model = keras_model(inputs = inputs, outputs = outputs)
```


```{r}
callbacks = list(callback_early_stopping(patience = patience),
                 callback_model_checkpoint(filepath=paste0('filename.h5'), monitor='val_loss', save_best_only=TRUE))

patience = 10
learning_rate = 0.1
epochs = 100

model %>% compile(optimizer = optimizer_sgd(lr = learning_rate),loss = "mse")
model %>% fit(x_train,x_train, epochs = epochs, batch_size = 100, validation_split = 0.1, callbacks = callbacks, verbose = 1)
k_clear_session()
load_model_hdf5(filepath = "filename.h5")
```

