{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Machine Learning for Python - Automatic Machine Learning\n",
    "Oracle Machine Learning for Python (OML4Py), a component of the Oracle Advanced Analytics option to Oracle Database Enterprise Edition, makes the open source Python scripting language and environment ready for the enterprise and big data. Designed for problems involving both large and small volumes of data, Oracle Machine Learning for Python integrates Python with Oracle Database, allowing users to execute Python commands and scripts for statistical, machine learning, and graphical analyses on database tables and views using Python syntax. Many familiar Python functions are overloaded and translate Python functions into SQL for in-database execution, as well as new automated machine learning capabilities. \n",
    "![title](img/OML4P_icon.jpg)\n",
    "In this notebook, we highlight using OML4Py Automated Machine Learning capability - AutoML, which consists of three key features:\n",
    "* Auto Feature Selection\n",
    "  * Reduce the number of features by identifying most predictive\n",
    "  * Improve performance and/or accuracy of the resulting model\n",
    "* Auto Model Selection for classification and regression\n",
    "  * Identify the best algorithm to achieve maximum accuracy metric\n",
    "  * Find best model many times faster than with exhaustive search techniques\n",
    "* Auto Tuning of Hyperparameters\n",
    "  * Significantly improve model accuracy\n",
    "  * Tune model many times faster than with manual or exhaustive search techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Oracle Database\n",
    "To use OML4Py, first import the package ***oml***. OML4Py supports a variety of connection specification options, including Oracle Wallet. Once connected to an Oracle Database that has OML4Py installed, invoking ***oml.isconnected*** returns true. \n",
    "\n",
    "The ***automl*** parameter in function ***connect***, when set to true, initializes settngs required for the oml.automk modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import oml\n",
    "from oml import automl\n",
    "from oml import algo\n",
    "from oml.automl import FeatureSelection\n",
    "\n",
    "oml.connect(\"pyquser2\",\"pyquser2\",\n",
    "            '(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521))(CONNECT_DATA=(service_name=OAA1)))',\n",
    "           automl=True)\n",
    "oml.isconnected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Pandas DataFrame and load into Oracle Database\n",
    "In this example, we load the iris data and combine target and predictors into a single DataFrame, which matches the form the data would have as a database table. This DataFrame is then loaded into Oracle Database using the ***create*** function, which creates a persistent table. \n",
    "\n",
    "For AutoML functionality, note that the target must be numeric, oven though the target column is categorical. In previous examples, we mapped the numeric species values to their string equivalents. Here, we leave it as numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (150, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   SEPAL_LENGTH  SEPAL_WIDTH  PETAL_LENGTH  PETAL_WIDTH  Species\n",
       "0           5.1          3.5           1.4          0.2        0\n",
       "1           4.9          3.0           1.4          0.2        0\n",
       "2           4.7          3.2           1.3          0.2        0\n",
       "3           4.6          3.1           1.5          0.2        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = pd.DataFrame(iris.data, \n",
    "                 columns = [\"SEPAL_LENGTH\", \"SEPAL_WIDTH\", \"PETAL_LENGTH\", \"PETAL_WIDTH\"])\n",
    "y = pd.DataFrame(iris.target,\n",
    "                 columns = ['Species']) # note that target must be numeric, despite categorical\n",
    "#y = pd.DataFrame(list(map(lambda x: {0:'setosa', 1: 'versicolor', 2:'virginica'}[x], iris.target)), \n",
    "#                 columns = ['Species'])\n",
    "iris_df = pd.concat([x,y], axis=1)\n",
    "\n",
    "oml.drop(table=\"IRIS\")\n",
    "IRIS = oml.create(iris_df, table=\"IRIS\")\n",
    "print(\"Shape:\",IRIS.shape)\n",
    "IRIS.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the proxy object for the database table by invoking ***sync*** and supplying the table name. Next, we split the data into train and test and prepare the train_x and train_y data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat, test_dat = oml.sync(table = \"IRIS\").split()\n",
    "train_x = train_dat.drop('Species')\n",
    "train_y = train_dat['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FeatureSelection',\n",
       " 'IRIS',\n",
       " 'In',\n",
       " 'Out',\n",
       " '_',\n",
       " '_1',\n",
       " '_2',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'algo',\n",
       " 'automl',\n",
       " 'datasets',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'iris',\n",
       " 'iris_df',\n",
       " 'linear_model',\n",
       " 'oml',\n",
       " 'pd',\n",
       " 'quit',\n",
       " 'test_dat',\n",
       " 'train_dat',\n",
       " 'train_x',\n",
       " 'train_y',\n",
       " 'warnings',\n",
       " 'x',\n",
       " 'y']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic feature selection using the 'accuracy' metric\n",
    "Automatic feature selection uses a technique called _meta-learning_ to quickly identify the most relevant features, or _predictors_, given a training data set and a machine learning technique, also referred to as a _mining function_, such as _classification_ or _regression_.\n",
    "\n",
    "In this example, we use the scoring metric 'accuracy'  to find the best features for classification. After creating a FeatureSelection object, the function ***reduce*** produces the selected features for the given data set and algorithm. We see that a single column, PETAL_WIDTH, was selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: ['PETAL_WIDTH']\n"
     ]
    }
   ],
   "source": [
    "fs = automl.FeatureSelection(mining_function = 'classification', score_metric = 'accuracy')\n",
    "selected_features = fs.reduce('dt', train_x, train_y)\n",
    "train_x_reduced = train_x[:,selected_features]\n",
    "print(\"Selected columns:\",train_x_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection on digits data set\n",
    "Let's use a more interesting data set, _digits_, from the openml package. First, we import some needed items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the digits data set, we provide column names, create a _case_id_ column, and then create the database table DIGITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset digits -- shape (1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   CASE_ID  COL0  COL1  COL2  COL3  COL4  COL5  COL6  COL7  COL8   ...    \\\n",
       "0       42     0     0     0     0    12     5     0     0     0   ...     \n",
       "1       43     0     0     0     9    15    12     0     0     0   ...     \n",
       "2       44     0     0     9    16    16    16     5     0     0   ...     \n",
       "3       45     0     0     9    16    13     6     0     0     0   ...     \n",
       "4       46     0     1    15     4     0     0     0     0     0   ...     \n",
       "\n",
       "   COL55  COL56  COL57  COL58  COL59  COL60  COL61  COL62  COL63  TARGET  \n",
       "0      0      0      0      0      3     16      8      0      0       1  \n",
       "1      0      0      0      0     11      7      0      0      0       7  \n",
       "2      0      0      0     13     10      0      0      0      0       7  \n",
       "3      0      0      0     11     14     12      8      0      0       3  \n",
       "4      0      0      0     14     14      4      0      0      0       5  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = load_digits()\n",
    "bc_data = bc.data.astype(float)\n",
    "print(\"Dataset {} -- shape {}\".format('digits', bc_data.shape))\n",
    "X = pd.DataFrame(bc_data, columns = [ 'COL{}'.format(i) for i in range(bc_data.shape[1]) ])\n",
    "y = pd.DataFrame(bc.target, columns = ['TARGET'])\n",
    "\n",
    "row_id_col = np.arange(bc_data.shape[0])\n",
    "row_id = pd.DataFrame(row_id_col, columns = ['CASE_ID'])\n",
    "\n",
    "try:\n",
    "    oml.drop(table='DIGITS')\n",
    "except:\n",
    "    pass\n",
    "DIGITS = oml.create(pd.concat([row_id, X, y], axis=1), table = 'DIGITS')\n",
    "DIGITS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the DIGITS data (in-database) using an 80/20 split, with a seed for reproducibility, producing the training and test data. Then, we specify the FeatureSelection object, and invoke ***reduce*** to select the features using the linear Support Vector Machine algorithm. Note that we can specify the degree of parallelism using the 'parallel' argument when creating the FeatureSelection object. \n",
    "\n",
    "Feature selection reduced the 64 columns to 45. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# features selected: 45 from 65\n"
     ]
    }
   ],
   "source": [
    "train_dat, test_dat = DIGITS.split(ratio=(0.8, 0.2), seed = 1234, \n",
    "                                   hash_cols = 'CASE_ID', \n",
    "                                   strata_cols='TARGET')\n",
    "\n",
    "train_x, train_y = train_dat.drop('TARGET'), train_dat['TARGET']\n",
    "test_x, test_y = test_dat.drop('TARGET'), test_dat['TARGET']\n",
    "\n",
    "fs = FeatureSelection(mining_function='classification', score_metric='accuracy', parallel=4)\n",
    "\n",
    "selected_features = fs.reduce('svm_linear', train_x, train_y, case_id='CASE_ID')\n",
    "\n",
    "print(\"# features selected: {} from {}\".format(len(selected_features), train_x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare improvement with Feature Selection results\n",
    "How does a model built using the selected features compare with one built on all the features? We'll build a model on the training data and score the test data - first using all columns from the original data set, and then on the reduced feature data set. We then compare the results for both speed and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = algo.svm(mining_function='classification')\n",
    "\n",
    "start_time = time.time()\n",
    "mod.fit(train_x, train_y)\n",
    "score = mod.score(test_x, test_y)\n",
    "no_fs_time = time.time() - start_time\n",
    "\n",
    "train_x_reduced  = train_x[:,selected_features]\n",
    "test_x_n = test_x[:,selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_reduced = algo.svm(mining_function='classification')\n",
    "\n",
    "start_time = time.time()\n",
    "mod_reduced.fit(train_x_reduced, train_y)\n",
    "score_reduced = mod_reduced.score(test_x_n, test_y)\n",
    "after_fs_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see below, accuracy is about the same, but time is reduced. But let's try a larger data set so the benefits become more obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature reduction 1.40x\n",
      "\n",
      "Accuracy Score: \n",
      "\tWithout FS: 0.95\n",
      "\tWith FS: 0.95\n",
      "\n",
      "Fit-time \n",
      "\tWithout FS: 3.73s, \n",
      "\tWith FS: 2.03s\n",
      "\tImprovement: 45.6%\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature reduction {:0.2f}x\\n\\n\".format(float(X.shape[1]-1)/len(selected_features)) +\n",
    "      \"Accuracy Score: \\n\\tWithout FS: {:0.2}\\n\\t\".format(score) +\n",
    "      \"With FS: {:0.2}\\n\".format(score_reduced))\n",
    "\n",
    "print(\"Fit-time \\n\\tWithout FS: {:0.2f}s, \\n\\tWith FS: {:0.2f}s\\n\\tImprovement: {:0.1f}%\".format(no_fs_time, \n",
    "                                                                                             after_fs_time,\n",
    "                                                                                            (no_fs_time-after_fs_time)/no_fs_time*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection on 554 data set\n",
    "Using the OPENML data set '554', this example highlights the performance and accuracy impact of feature selection on a larger table involving 70K rows and 786 columns. This corresponds to the NMIST_784 data set of hand-written digits. \n",
    "\n",
    "As before, we prepare the data and create the database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape (70000, 784)\n",
      "CPU times: user 1min 30s, sys: 59.8 s, total: 2min 30s\n",
      "Wall time: 2min 48s\n",
      "Data Table Shape: (70000, 786)\n"
     ]
    }
   ],
   "source": [
    "# ds = openml.datasets.get_dataset('554')\n",
    "# n_arr, col_names = ds.get_data(return_attribute_names=True)\n",
    "# feats, y = n_arr[:,:-1], n_arr[:,-1]\n",
    "# print(\"Dataset shape {}\".format(feats.shape))\n",
    "# \n",
    "# row_id_col = np.arange(feats.shape[0]).reshape(feats.shape[0], -1)\n",
    "# X = np.hstack((feats, row_id_col))\n",
    "# col_names[-1] = 'CASE_ID'\n",
    "# \n",
    "# X = pd.DataFrame(X, columns = col_names)\n",
    "# y = pd.DataFrame(y, columns = ['TARGET'])\n",
    "# \n",
    "# try:\n",
    "#     oml.drop(table='OPENML_554')\n",
    "# except:\n",
    "#     pass\n",
    "# \n",
    "# %time OPENML_554 = oml.create(pd.concat([X, y], axis=1), table = 'OPENML_554')\n",
    "\n",
    "OPENML_554 = oml.sync(table='OPENML_554')\n",
    "print(\"Data Table Shape:\",OPENML_554.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 21 ms, total: 172 ms\n",
      "Wall time: 19.8 s\n",
      "# features selected: 45 from 65\n"
     ]
    }
   ],
   "source": [
    "fs = FeatureSelection(mining_function='classification', score_metric='accuracy', parallel=4)\n",
    "\n",
    "%time selected_features = fs.reduce('svm_gaussian', train_x, train_y, case_id='CASE_ID')\n",
    "\n",
    "print(\"# features selected: {} from {}\".format(len(selected_features), train_x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for the DIGITS data set, we compare the results for accuracy and performance, using the SVM Gaussian algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 ms, sys: 0 ns, total: 55 ms\n",
      "Wall time: 3.48 s\n",
      "CPU times: user 13 ms, sys: 2 ms, total: 15 ms\n",
      "Wall time: 225 ms\n"
     ]
    }
   ],
   "source": [
    "def get_svm_gaussian():\n",
    "    mod = algo.svm(mining_function='classification')\n",
    "    mod.set_params(SVMS_KERNEL_FUNCTION='SVMS_GAUSSIAN')\n",
    "    return mod\n",
    "\n",
    "mod = get_svm_gaussian()\n",
    "\n",
    "start_time = time.time()\n",
    "%time mod.fit(train_x, train_y)\n",
    "%time score = mod.score(test_x, test_y)\n",
    "no_fs_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 129 ms, sys: 7 ms, total: 136 ms\n",
      "Wall time: 4.76 s\n",
      "CPU times: user 52 ms, sys: 7 ms, total: 59 ms\n",
      "Wall time: 431 ms\n"
     ]
    }
   ],
   "source": [
    "train_x_reduced  = train_x[:,selected_features]\n",
    "test_x_n = test_x[:,selected_features]\n",
    "\n",
    "mod_reduced = get_svm_gaussian()\n",
    "\n",
    "start_time = time.time()\n",
    "%time mod_reduced.fit(train_x_reduced, train_y)\n",
    "%time score_reduced = mod_reduced.score(test_x_n, test_y)\n",
    "after_fs_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature reduction 1.42x\n",
      "\n",
      "Accuracy Score\n",
      "\tWithout FS: 0.98\n",
      "\tWith FS: 0.98\n",
      "\n",
      "Fit-time\n",
      "\tWithout FS: 3.71s, \n",
      "\tWith FS: 5.20s\n",
      "\tImprovement: -40.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature reduction {:0.2f}x\\n\\n\".format(float(train_x.shape[1]-1)/len(selected_features)) +\n",
    "      \"Accuracy Score\\n\\tWithout FS: {:0.2f}\\n\".format(score) +\n",
    "      \"\\tWith FS: {:0.2f}\\n\".format(score_reduced))\n",
    "\n",
    "print(\"Fit-time\\n\\tWithout FS: {:0.2f}s, \\n\\tWith FS: {:0.2f}s\\n\\tImprovement: {:0.1f}%\".format(no_fs_time, \n",
    "                                                                                             after_fs_time,\n",
    "                                                                                            (no_fs_time-after_fs_time)/no_fs_time*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature and Model Selection using 'BreastCancer' data set\n",
    "In this example, we use another data set to illustrate both feature and model selection. For feature selection, we show the difference in selected columns based on the chosen metric. \n",
    "\n",
    "As before, we create the database table, then split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (569, 31)\n"
     ]
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "bc_data = bc.data.astype(float)\n",
    "X = pd.DataFrame(bc_data, columns = bc.feature_names)\n",
    "y = pd.DataFrame(bc.target, columns = ['TARGET'])\n",
    "\n",
    "try:\n",
    "    oml.drop(table='BREASTCANCER')\n",
    "except:\n",
    "    pass\n",
    "BREASTCANCER = oml.create(pd.concat([X, y], axis=1), table = 'BREASTCANCER')\n",
    "print(\"Shape:\",BREASTCANCER.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat, test_dat = oml.sync(table = \"BREASTCANCER\").split()\n",
    "test_x, test_y = test_dat.drop('TARGET'), test_dat['TARGET']\n",
    "train_x = train_dat.drop('TARGET')\n",
    "train_y = train_dat['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define FeatureSelection objects - one with metric 'accuracy', and one with metric 'f1_macro'.\n",
    "\n",
    "Notice that the 'accuracy' metric selected significantly fewer features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: ['mean compactness', 'mean concave points', 'mean fractal dimension', 'radius error', 'smoothness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst fractal dimension']\n",
      "Number of columns: 10\n"
     ]
    }
   ],
   "source": [
    "fs = automl.FeatureSelection(mining_function = 'classification', score_metric = 'accuracy')\n",
    "selected_features = fs.reduce('dt', train_x, train_y)\n",
    "train_x_reduced = train_x[:,selected_features]\n",
    "print(\"Selected columns:\",train_x_reduced.columns)\n",
    "print(\"Number of columns:\", train_x_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: ['mean radius', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'smoothness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst smoothness', 'worst concavity', 'worst concave points', 'worst fractal dimension']\n",
      "Number of columns: 16\n"
     ]
    }
   ],
   "source": [
    "fs = automl.FeatureSelection(mining_function = 'classification', score_metric = 'f1_macro')\n",
    "selected_features = fs.reduce('dt', train_x, train_y)\n",
    "train_x_reduced = train_x[:,selected_features]\n",
    "print(\"Selected columns:\",train_x_reduced.columns)\n",
    "print(\"Number of columns:\", train_x_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the best model\n",
    "For AutoML model selection, we create a ModelSelection object, here with the 'f1_macro' score metric. Then, we invoke ***select*** to get the top predicted algorithm by setting k=1. This defaults to the tuned model, which is then displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = automl.ModelSelection(mining_function='classification', \n",
    "                           score_metric='f1_macro', parallel=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _ExtRef.__del__ of <oml.core.extref._ExtRef object at 0x7f43ba90bf98>>\n",
      "Traceback (most recent call last):\n",
      "  File \"oml/core/extref.py\", line 31, in oml.core.extref._ExtRef.__del__\n",
      "Exception ignored in: <bound method _ExtRef.__del__ of <oml.core.extref._ExtRef object at 0x7f43ba90bf98>>\n",
      "  File \"oml/core/extref.py\", line 47, in oml.core.extref._ExtRef.__dropObjects\n",
      "cx_Oracle.DatabaseError: ORA-01001: invalid cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"oml/core/extref.py\", line 31, in oml.core.extref._ExtRef.__del__\n",
      "  File \"oml/core/extref.py\", line 47, in oml.core.extref._ExtRef.__dropObjects\n",
      "cx_Oracle.DatabaseError: ORA-01001: invalid cursor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 1.1 s, total: 2.61 s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Algorithm Name: Support Vector Machine\n",
       "\n",
       "Mining Function: CLASSIFICATION\n",
       "\n",
       "Target: TARGET\n",
       "\n",
       "Settings: \n",
       "                    setting name                 setting value\n",
       "0                      ALGO_NAME  ALGO_SUPPORT_VECTOR_MACHINES\n",
       "1          CLAS_WEIGHTS_BALANCED                           OFF\n",
       "2                   ODMS_DETAILS                  ODMS_DISABLE\n",
       "3   ODMS_MISSING_VALUE_TREATMENT       ODMS_MISSING_VALUE_AUTO\n",
       "4                  ODMS_SAMPLING         ODMS_SAMPLING_DISABLE\n",
       "5                      PREP_AUTO                            ON\n",
       "6         SVMS_COMPLEXITY_FACTOR                            10\n",
       "7            SVMS_CONV_TOLERANCE                         .0001\n",
       "8           SVMS_KERNEL_FUNCTION                 SVMS_GAUSSIAN\n",
       "9                SVMS_NUM_PIVOTS                           200\n",
       "10                  SVMS_STD_DEV            5.3999999999999995\n",
       "\n",
       "Attributes: \n",
       "mean radius\n",
       "mean texture\n",
       "mean perimeter\n",
       "mean area\n",
       "mean smoothness\n",
       "mean compactness\n",
       "mean concavity\n",
       "mean concave points\n",
       "mean symmetry\n",
       "mean fractal dimension\n",
       "radius error\n",
       "texture error\n",
       "perimeter error\n",
       "area error\n",
       "smoothness error\n",
       "compactness error\n",
       "concavity error\n",
       "concave points error\n",
       "symmetry error\n",
       "fractal dimension error\n",
       "worst radius\n",
       "worst texture\n",
       "worst perimeter\n",
       "worst area\n",
       "worst smoothness\n",
       "worst compactness\n",
       "worst concavity\n",
       "worst concave points\n",
       "worst symmetry\n",
       "worst fractal dimension\n",
       "\n",
       "Partition: NO\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time selected_model = ms.select(train_x, train_y, k=1)\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run model selection, but turn off tuning so we can see the 'f1_macro' metrics produced by each model. Here we see explicitly that SVM Gaussian produced the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 295 ms, sys: 134 ms, total: 429 ms\n",
      "Wall time: 25.4 s\n",
      "Ranked models:\n",
      " [('svm_gaussian', 0.9683665987729823), ('nn', 0.9587999707195006), ('svm_linear', 0.9529611650687159)]\n"
     ]
    }
   ],
   "source": [
    "%time ranked_models = ms.select(train_x, train_y, tune=False)\n",
    "\n",
    "print(\"Ranked models:\\n\",ranked_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "Using the training data from the BREASTCANCER table created above, we define a ModelTuning object for classification and specify degree of parallelism at 4. We then invoke ***run*** to produce the tuned model using the decision tree algorithm.\n",
    "\n",
    "The result of model tuning is a dictionary with the best model and the tuned parameters. The model can be used for scoring using the ***predict*** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oml.automl import ModelTuning\n",
    "\n",
    "at = ModelTuning(mining_function = 'classification', parallel=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 421 ms, sys: 92 ms, total: 513 ms\n",
      "Wall time: 37.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Algorithm Name: Decision Tree\n",
       "\n",
       "Mining Function: CLASSIFICATION\n",
       "\n",
       "Target: TARGET\n",
       "\n",
       "Settings: \n",
       "                    setting name            setting value\n",
       "0                      ALGO_NAME       ALGO_DECISION_TREE\n",
       "1              CLAS_MAX_SUP_BINS                       32\n",
       "2          CLAS_WEIGHTS_BALANCED                      OFF\n",
       "3                   ODMS_DETAILS             ODMS_DISABLE\n",
       "4   ODMS_MISSING_VALUE_TREATMENT  ODMS_MISSING_VALUE_AUTO\n",
       "5                  ODMS_SAMPLING    ODMS_SAMPLING_DISABLE\n",
       "6                      PREP_AUTO                       ON\n",
       "7           TREE_IMPURITY_METRIC    TREE_IMPURITY_ENTROPY\n",
       "8            TREE_TERM_MAX_DEPTH                        7\n",
       "9          TREE_TERM_MINPCT_NODE                     0.05\n",
       "10        TREE_TERM_MINPCT_SPLIT                      0.1\n",
       "11         TREE_TERM_MINREC_NODE                       10\n",
       "12        TREE_TERM_MINREC_SPLIT                       20\n",
       "\n",
       "Attributes: \n",
       "mean radius\n",
       "mean texture\n",
       "mean perimeter\n",
       "mean area\n",
       "mean smoothness\n",
       "mean compactness\n",
       "mean concavity\n",
       "mean concave points\n",
       "mean symmetry\n",
       "mean fractal dimension\n",
       "radius error\n",
       "texture error\n",
       "perimeter error\n",
       "area error\n",
       "smoothness error\n",
       "compactness error\n",
       "concavity error\n",
       "concave points error\n",
       "symmetry error\n",
       "fractal dimension error\n",
       "worst radius\n",
       "worst texture\n",
       "worst perimeter\n",
       "worst area\n",
       "worst smoothness\n",
       "worst compactness\n",
       "worst concavity\n",
       "worst concave points\n",
       "worst symmetry\n",
       "worst fractal dimension\n",
       "\n",
       "Partition: NO\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time results = at.run('dt', train_x, train_y, score_metric='accuracy')\n",
    "\n",
    "tuned_model = results['best_model']\n",
    "tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   PREDICTION\n",
       "0           1\n",
       "1           0\n",
       "2           1\n",
       "3           0\n",
       "4           1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = tuned_model.predict(test_x)\n",
    "pred_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show tuned model score metric and tuned hyperparameters. Use the results to get the score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.91',\n",
       " ['CLAS_MAX_SUP_BINS:32',\n",
       "  'TREE_IMPURITY_METRIC:TREE_IMPURITY_ENTROPY',\n",
       "  'TREE_TERM_MAX_DEPTH:7',\n",
       "  'TREE_TERM_MINPCT_NODE:0.05',\n",
       "  'TREE_TERM_MINPCT_SPLIT:0.1'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, params = results['all_evals'][0]\n",
    "\"{:.2}\".format(score), [\"{}:{}\".format(k, params[k]) for k in sorted(params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.91'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:.2}\".format(tuned_model.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can also invoke model tuning with user-defined search ranges for selected hyperparameters on a tuning metric, which in this example is 'f1_macro'.\n",
    "\n",
    "Here, we use the Random Forest algorothm specified by 'rf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.94',\n",
       " ['RFOR_NUM_TREES:53',\n",
       "  'RFOR_SAMPLING_RATIO:0.35',\n",
       "  'TREE_IMPURITY_METRIC:TREE_IMPURITY_GINI'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space={'RFOR_SAMPLING_RATIO': {'type': 'continuous', \n",
    "                                      'range': [0.05, 0.5]}, \n",
    "              'RFOR_NUM_TREES': {'type': 'discrete', 'range': [50, 55]}, \n",
    "              'TREE_IMPURITY_METRIC': {'type': 'categorical', \n",
    "                                       'range': ['TREE_IMPURITY_ENTROPY', \n",
    "                                                 'TREE_IMPURITY_GINI']},}\n",
    "\n",
    "results = at.run('rf', train_x, train_y, score_metric='f1_macro', param_space=search_space)\n",
    "\n",
    "score, params = results['all_evals'][0]\n",
    "(\"{:.2}\".format(score), [\"{}:{}\".format(k, round(params[k], 3) if isinstance(params[k], float) else params[k]) for k in sorted(params)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyperparameter search ranges, such as Random Forest's MTRY argument, need to be defined based on the training dataset sizes, e.g., number of samples or features. RFOR_MTRY sets the size of the random subset of columns to be considered when choosing a split at a node. The data set-specific placeholders like ***nr_features*** or ***nr_samples*** can be used for this purpose as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.94', ['RFOR_MTRY:10'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space={'RFOR_MTRY': {'type': 'discrete', \n",
    "                            'range': [1, '$nr_features/2']}}\n",
    "\n",
    "results = at.run('rf', train_x, train_y, score_metric='f1_macro', param_space=search_space)\n",
    "\n",
    "score, params = results['all_evals'][0]\n",
    "(\"{:.2}\".format(score), [\"{}:{}\".format(k, params[k]) for k in sorted(params)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Oracle-sm.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
