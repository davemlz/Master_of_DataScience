---
title: "Técnicas de agrupamiento"
subtitle: "Scripts de ejemplo, acompañamiento de la Teoría"
author: "Santander Meteorology Group"
output:
  html_document:
    fig_caption: yes
    highlight: pygments
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    fig_caption: yes
    highlight: pygments
    latex_engine: pdflatex
    pandoc_args:
    - --number-sections
    - --number-offset=0
    toc: yes
encoding: UTF8
documentclass: article
abstract: Se presentan una serie de breves scripts de ejemplo que acompañan a la parte teórica en el aula sobre técnicas de agrupamiento.
urlcolor: blue
---

\fontfamily{cmr}
\fontsize{11}{22}
\selectfont

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      highlight = TRUE,
                      message = FALSE,
                      fig.align = "center",
                      tidy = FALSE,
                      fig.width = 7,
                      cache = TRUE)
```

# Introducción y paquetes necesarios

Se desarrollarán una serie de ejemplos utilizando el famoso conjunto de datos [iris](https://archive.ics.uci.edu/ml/datasets/iris), y algunos paquetes enumerado a continuación:

```{r}
library(MASS)
library(mclust)
library(caret)
data(iris)
```
El paquete `magrittr` se utilizará por conveniencia, al incorporar (entre otros) el operador `%>%` que permite concatenar acciones de manera sencilla. 

```{r}
require(magrittr)
```

# Gaussian Mixtures

Esta técnica de agrupamiento se encuentra implementada en el paquete `mclust`. El ajuste del modelo es muy sencillo con la función `Mclust`:

```{r}
gmModel <- Mclust(iris[,-5])
```
Los objetos de la clase `Mclust` cuentan con un método de resumen:

```{r}
summary(gmModel)
```
y un método `plot`:

```{r}
plot(gmModel)
```


La función plot inicia un menú interactivo que permite visualizar diferentes tipos de gráficos (el valor 0 permite salir del menú). Estos valores también pueden especificarse mediante el argumento `what`.

 * `"BIC"`: plot of BIC values used for choosing the number of clusters
 * `"classification"`: A plot showing the clustering. For data in more than two dimensions a pairs plot is produced, followed by a coordinate projection plot using specified dimens. Ellipses corresponding to covariances of mixture components are also drawn if `addEllipses = TRUE`.
 * `"uncertainty"`: a plot of classification uncertainty. For data in more than two dimensions a coordinate projection plot is drawn using specified dimension
 * `"density"`: a plot of estimated density. For data in more than two dimensions a matrix of contours for coordinate projection plot is drawn using specified dimens.

## Ejemplo: Evaluando la capacidad de separación.

En este ejemplo se indica explícitamente el numero de grupos (_mixture components_) a clasificar mediante el argumento `G`.

```{r}
gmModel <- Mclust(iris[,-5])
summary(gmModel)
plot(gmModel, what = "classification") 
```

El algoritmo EM, sin ninguna indicación por nuestra parte, encuentra dos grupos diferenciados. Comprobamos a continuación la capacidad del método para reconocer las tres especies de _iris_ que se encuentran el el conjunto de datos.

```{r}
## Defining the number of G-M (mixture components)
gmModel <- Mclust(iris[,-5], G = 3)
summary(gmModel)
## Comparing observation and prediction
plot(gmModel, what = "density")
par(mfrow = c(1,2))
plot(iris[ ,c(1,3)], col = iris[,5], main = "Observed", asp = 1)
legend("topleft", c(levels(iris[,5])), pch = 21, col = 1:3)
plot(iris[ ,c(1,3)], col = gmModel$classification, main = "Predicted")
mtext("Gaussian Mixture Model , G=3")
```
A simple vista, el agrupamiento realiza un buen trabajo separando las tres especies. Comprobamos a continuación el error cometido:

```{r}
tc <- confusionMatrix(as.factor(as.numeric(iris[,5])),
                      as.factor(gmModel$classification)) %>% print()
```

Como puede verse, la clasificacion es bastante precisa, con un accuracy de `r round(tc$overall[1],3)`.


# K-medias

Ejemplo animado:
https://thumbs.gfycat.com/SoftEnragedHypsilophodon-mobile.mp4

## Comparativa con el método anterior

Una primera exploración del método permite ver que para este caso particular, parece menos adecuado que Gaussian Mixture, debido a que es un método basado en distancias, menos apropiado para este conjunto de datos en particular:


```{r}
set.seed(53)
kmModel <- kmeans(iris[ ,-5], 3, nstart = 1)
par(mfrow = c(1,2))
plot(iris[ ,c(1,3)], col = iris[,5], main = "Observed", asp = 1)
plot(iris[,c(1,3)], type = "n", asp = 1, main = "Predicted")
mtext("K-Means Clustering , K=3")
for (i in 1:3) {
    points(iris[which(kmModel$cluster == i), c(1,3)], col = i, pch = 21)
}
```

```{r}
tckm <- confusionMatrix(as.factor(as.numeric(iris[,5])),
                        as.factor(kmModel$cluster)) %>% print()
```

Efectivamente, en este caso la accuracy baja de `r round(tc$overall[1], 3)` obtenida en Gaussian Mixture a `r round(tckm$overall[1], 3)`.

Sin embargo, debido a su relativa simplicidad y buen rendimiento en general, sigue siendo uno de los métodos de agrupamiento más utilizados. 

## Inicialización del algoritmo

El resultado depende del inicio del algoritmo. Como los primeros puntos son aleatorios, los resultados varían
entre realizaciones:

```{r}
par(mfrow = c(2,3))
j <- 6 
while (j > 0) {
    set.seed(j)
    j <- j - 1
    km <- kmeans(iris[,-5], centers = 3, nstart = 1)
    plot(iris[ ,c(1,3)], type = "n")
    for (i in 1:3) {
        points(iris[km$cluster == i,c(1,3)], pch = 19, col = i)
    }
    mtext(format(km$tot.withinss, digits = 2))
}
```

Para conseguir un óptimo global en la separación, la estrategia consiste en repetir el ciclo completo de iteraciones varias veces, comenzando con inicializaciones aleatorias cada vez. Esto se indica mediante el argumento `nstart`, de modo que tras el número indicado de iteraciones se retiene la que obtuvo una mejor separación (menor distancia intra-cluster y mayor distancia entre clusters).

```{r}
km20 <- kmeans(iris[,-5], centers = 3, nstart = 20)
plot(iris[ ,c(1,3)], type = "n")
mtext(format(km20$tot.withinss, digits = 2))
for (i in 1:3) {
    points(iris[km20$cluster == i,c(1,3)], pch = 19, col = i)
}
```


## Selección del valor de K

Los dos parámetros más importantes a la hora de evaluar la clasificación son la suma de cuadrados intra-cluster (_within-cluster sum of squares_), que da una idea de la dispersión de las observaciones dentro de cada uno de los grupos creados:

```{r}
kmModel$withinss ## Vector of within-cluster sum of squares, one component per cluster
```

Y la suma de cuadrados global, o entre grupos (_between-cluster sum of squares_), que proporciona una medida de la separación entre grupos diferentes, que se pretende maximizar.

```{r}
kmModel$betweenss ## The between-cluster sum of squares
```

Una de las característica de este método es que forma un número de grupos arbitrario, que debe indicar a priori el usuario. En el conjunto de datos `iris` del ejemplo, ya conocido, sabemos que existen tres subgrupos subyacentes en el conjunto de datos. Sin embargo a menudo desconocemos qué valor puede ser el adecuado. Puede ayudarnos a decidir la suma de cuadrados global para diferentes valores de K:

```{r}
max.K = 20
ss <- vapply(1:max.K, FUN.VALUE = numeric(1), FUN = function(k) {
    kmeans(x = iris[,-5], centers = k, nstart = 20) %>% extract2("betweenss")
})
plot(1:max.K, ss, ty = "b")
```

Ya que el algoritmo maximiza la distancia entre grupos en cada iteración, ésta aumenta con el valor de K. Sin embargo, este aumento es rápido al principio y progresivamente más lento. Resulta conveniente tomar un valor de K en un punto de la curva que forme un "codo" (_elbow_). En este caso, sin saber nada a priori del conjunto de datos, parecería razonable tomar K=4 o K=5 para la agrupación.

# SOM (self-organizing maps)

La SOM es un método similar a K-Means, pero introduce la noción de topología. La función `somgrid` permite permite definir una rejilla con una forma y tamaño determinados. Se muestran algunos ejemplos a continuación:

```{r}
library(kohonen)
## Definition of the topology.
som <- som(as.matrix(scale(iris[,-5])), somgrid(xdim = 6, ydim = 6, topo = "rectangular"))
## Should be used to visualize the data:
plot(som) ## Considering 3 classes
```

```{r}
somR <- som(as.matrix(scale(iris[,-5])), somgrid(xdim = 1, ydim = 3, topo = "rectangular"))
plot(somR)
```

```{r}
somH <- som(as.matrix(scale(iris[,-5])), somgrid(xdim = 3, ydim = 1, topo = "hexagonal"))
plot(somH)
## We obtain the classification:
confusionMatrix(as.factor(as.numeric(iris[,5])), as.factor(somH$unit.classif))
```

# (Centroid-based) Fuzzy clustering

Se encuentra implementado en el paquete `e1071`. Mientras que el método de K-medias establece grupos que no solapan, en el fuzzy clustering la idea es asignar a cada instancia una probabilidad de pertenencia a cada grupo

```{r}
library(e1071)
cmModel <- cmeans(iris[,-5], 3, iter.max = 1, m = 2, method = "cmeans")
summary(cmModel) ## Point center of two attributes
plot(iris[ ,c(1,3)], col = cmModel$cluster, main = "K-Means")
points(cmModel$centers[,c(1,3)], col = 1:3, pch = 8, cex = 2)
```

Si se analiza la tabla de contingencias resultante el resultado es bastante malo en comparación con los anteriores métodos:

```{r}
confusionMatrix(as.factor(as.numeric(iris[,5])), as.factor(cmModel$cluster))
```
Sin embargo, la ventaja de este método se encuentra en las probabilidades de pertenencia de cada observación, que se encuentra almacenadas dentro del elemento `"membership"`

```{r}
str(cmModel$membership)
```

Así, puede extraerse el más probable, tal y como nos lo devuelve el elemento `cmModel$cluster`

```{r}
apply(cmModel$membership, MARGIN = 1, FUN = "which.max")
```

Pero también representar gráficamente la solución de forma más informativa, considerando la probabilidad de pertenencia de cada observación a un grupo determinado

```{r}
centroids <- cmModel$centers[,c(1,3)]
```


```{r,fig.width=12}
library(classInt) # class-interval recoding library
library(RColorBrewer) # useful color palettes
nclr <- 10 # Number of intervals
colors <- colorRampPalette(rev(brewer.pal(nclr,"YlOrRd")))(10) # Interpolate color palette
colors <- colors[nclr:1] # reorder colors
## Centroids (to draw them in the plots)
centroids <- cmModel$centers[,c(1,3)]
par(mfrow = c(1,3))
for (i in 1:ncol(cmModel$membership)) {
  class <- classIntervals(cmModel$membership[, i], n = nclr, style = "quantile")
  colcode <- findColours(class, colors)
  plot(iris[ ,c(1,3)], pch = 19,
       col = colcode,
       xlab = names(iris)[1],
       ylab = names(iris)[3],
       main = paste("Probability of belonging to ", levels(iris$Species)[i]," sp."))
  points(centroids[i, 1], centroids[i, 2], cex = 3, pch = 8, col = "purple")
  legend("topleft", names(attr(colcode, "table")),
         pch = 19, col = attr(colcode, "palette"),
         cex = .75, bty = "n")
}
```


Como se ha visto en anteriores métodos, la separación de las especies _versicolor_ y _virginica_ es complicada, quedando los centroides de ambos clusters muy próximos entre sí, y claramente diferenciados de _setosa_, que tiene claramente un menor tamaño de la flor.


# Hierarchical clustering


## Aglomerativo

```{r}
d <- dist(iris[ ,-5], method = "euclidean")
## Available linkage criterion: "ward.D","ward.D2","single","complete","average", "mcquitty","median" or "centroid".
par(mfrow = c(2,2))
plot(hclust(d, method = "complete"), main = "CompleteLinkage", col = "blue", axes = FALSE)
plot(hclust(d, method = "single"), main = "SingleLinkage", col = "red",axes = FALSE)
plot(hclust(d, method = "average"), main = "AverageLinkage", col = "green", axes = FALSE)
plot(hclust(d, method = "centroid"), main = "CentroidLinkage", col = "black", axes = FALSE)
```


## Divisivo

```{r}
library(cluster)
d <- dist(iris[,-5], method = "euclidean")
diModel <- diana(d, diss = TRUE, metric = "euclidean")
plot(diModel)
```

El número de grupos se indica mediante la opción `cutree`, cortando el árbol a la altura conveniente. En este caso, para obtener tres grupos

```{r}
di3 <- cutree(tree = diModel, k = 3) ## 3 classes
```

Comparamos a continuación la efectividad del clustering jerárquico divisivo:

```{r}
confusionMatrix(as.factor(as.numeric(iris[,5])), as.factor(di3))
```

con la del clustering jerárquico aglomerativo:

```{r}
hc3 <- hclust(d, method = "complete") %>% cutree(k = 3)
confusionMatrix(as.factor(as.numeric(iris[,5])), as.factor(hc3))
```

Los resultados revelan una mejor separación de clases mediante la técnica divisiva ($accuracy \approx 0.9$ frente a $accuracy \approx 0.5$)

## Interpretación del dendrograma.

En esta sección utilizaremos el ejemplo mostrado en la figura siguiente, donde hay 45 observaciones que pertencen a grupos bien diferenciados. El clustering jerárquico (usando el método _complete linkage_) realiza la agrupación que representaremos a continuación. ¿Cómo interpretamos el dendrograma?


```{r}
set.seed(2)
x <- matrix(rnorm(45 * 2), ncol = 2)
x[1:15, 1] <- x[1:15, 1] + 2
x[1:15, 2] <- x[1:15, 2] + 2
x[16:30, 2] <- x[16:30, 2] - 4
x[16:30, 1] <- x[16:30, 1] - 2
x[31:45, 1] <- x[31:45, 1] - 4
x[31:45, 2] <- x[31:45, 2] + 1 
plot(x, ty = "n", xlab = "")
points(x[1:15, ], col = 2, pch = 19)
points(x[16:30, ], col = 3, pch = 19)
points(x[31:45, ], col = 4, pch = 19)
```
En el dendrograma de la figura están representados los datos de la figura , pero coloreados para diferentes niveles de agrupamiento. Los extremos inferiores (las hojas) corresponden a cada una de las observaciones. A medida que ascendemos por el árbol, algunas hojas comienzan a unirse para dar lugar a ramas. Estas observaciones son las más similares entre sí. A medida que nos movemos hacia arriba en el árbol, las ramas se van uniendo entre sí o con otras hojas. Cuanto antes se unen, más similitud hay entre los grupos de observaciones. Por el contrario, cuanto más lejana es la unión entre ramas, más alejados se encuentran entre sí los grupos de observaciones. La altura de estas uniones, medida en el eje vertical del dendrograma, mide la diferencia entre dos observaciones.

```{r}
require(sparcl) # colorea las hojas del dendrograma de forma facil
hc <- hclust(dist(x), method = "complete")
par(mfrow = c(1,3))
y1 <- cutree(hc, k = 1)
ColorDendrogram(hc, y = y1, ylab = "", xlab = "", branchlength = 10)
y2 <- cutree(hc, h = 11)
ColorDendrogram(hc, y = y2, ylab = "", xlab = "", branchlength = 10)
abline(h = 11, lty = 2)
y3 <- cutree(hc, h = 6)
ColorDendrogram(hc, y = y3, ylab = "", xlab = "", branchlength = 10)
abline(h = 6, lty = 2)
```


NOTA: No pueden extraerse conclusiones sobre la similitud entre dos observaciones por su posición en el eje horizontal. Por el contrario, debemos observar la primera unión de ambas observaciones y su punto de corte con el eje vertical. A modo de ejemplo, se presenta la figura : Los pares de observaciones (9,8) y (2,7) son muy similares entre sí (la distancia euclídea que los separa es pequeña, como puede verse en el plot de la izquierda). Sin embargo, la distancia entre (4,3) es considerable, y sin embargo aparecen seguidos en el eje horizontal. Puede comprobarse que su punto de unión en el eje vertical es el más alejado (>4). Por el contrario, las observaciones 1 y 7, alejadas entre sí en el eje horizontal, son mucho más próximas, cortando el eje vertical en un valor próximo a 2.

```{r}
# GENERA FIGURA 4
set.seed(1)
x <- matrix(rnorm(20), ncol = 2)
hc <- hclust(dist(x))
par(mfrow = c(1,2))
plot(x, ty = "n", xlab = "", ylab = "", asp = 1)
text(x[hc$order,1], x[hc$order,2], hc$order)
plot(hc, xlab = "", ylab = "", main = "", hang = -1)
```

### Identificación de grupos

Ahora que podemos interpretar un dendrograma, pasamos a analizar la identificación de los grupos que se han generado. Para ello, hacemos un corte horizontal en un punto dado del eje vertical, tal y como se indica con una línea punteada en los paneles central y derecho de la figura . Los grupos de observaciones definidos bajo la línea de corte son los clusters resultantes. Como se ve en la figura , en el panel central, al trazar un corte a la altura de 11 generamos dos clusters diferentes. A medida que descendemos, el número de clusters aumenta. En el panel derecho, vemos que para un nivel de corte de 6 obtenemos 3 clusters. Descendiendo, se obtienen progresivamente más grupos, hasta llegar a tantos grupos como observaciones para un nivel de corte de 0. Por tanto, la altura de corte sirve para definir el número de grupos, tal y como el parámetro K hacía en la técnica de las K-medias.

En la práctica, normalmente el número de grupos se suele seleccionar por inspección visual del dendrograma, buscándose un número de grupos razonable en función de las diferentes alturas y el número de clusters que se quiera obtener. No obstante, como se ve un único dendrograma puede servir para seleccionar distinto número de grupos. En el caso de la figura , el nivel de corte para obtener tres grupos parece el más razonable, sobretodo si tenemos en cuenta los datos de partida representados en la figura . Sin embargo, a veces la selección del número de grupos no es tan obvia. En este sentido, si tenemos un conocimiento a priori del tipo de grupos que podríamos encontrar, el agrupamiento jerárquico podría no ser la mejor opción.

### Funcionamiento del algoritmo

El dendrograma se obtiene mediante un algoritmo relativamente sencillo. Se comienza por definir algún tipo de medida de distancia entre cada par de observaciones (típicamente la distancia Euclídea, aunque hay muchas otras, (`help("dist")`). Se comienza desde la base del árbol, donde cada observación es tratada como un cluster independiente. A partir de ahí, de forma iterativa, se van uniendo los clusters más cercanos entre sí hasta llegar a un único cluster que engloba a todas las observaciones, momento en el que el dendrograma se completa y se para. Esto queda reflejado, para las 6 primeras iteraciones, en la figura, que se corresponde con los datos mostrados anteriormente en el dendrograma de la figura anterior.

```{r}
set.seed(1)
x <- matrix(rnorm(20), ncol = 2)
hc <- hclust(dist(x))
par(mfrow = c(2,3))
# plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 1")
# text(x[hc$order,1], x[hc$order,2], hc$order)
plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 1")
text(x[hc$order,1], x[hc$order,2], hc$order)
rect(0.5,0.7,0.9,1.1, border = "green", lwd = 2)

plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 2")
text(x[hc$order,1], x[hc$order,2], hc$order)
rect(0.5,0.7,0.9,1.1, border = "green", lwd = 2)
rect(0.22,0.67,0.95,1.3, border = "brown", lwd = 2)

plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 3")
text(x[hc$order,1], x[hc$order,2], hc$order)
rect(0.5,0.7,0.9,1.1, border = "green", lwd = 2)
rect(0.22,0.67,0.95,1.3, border = "brown", lwd = 2)
rect(0.1,-0.15,0.6,0.45, border = "purple", lwd = 2)

plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 4")
text(x[hc$order,1], x[hc$order,2], hc$order)
rect(0.5,0.7,0.9,1.1, border = "green", lwd = 2)
rect(0.22,0.67,0.95,1.3, border = "brown", lwd = 2)
rect(0.1,-0.15,0.6,0.45, border = "purple", lwd = 2)
rect(-0.95,-0.75,-0.6,0.15, border = "red", lwd = 2)

plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 5")
text(x[hc$order,1], x[hc$order,2], hc$order)
rect(0.5,0.7,0.9,1.1, border = "green", lwd = 2)
rect(0.22,0.67,0.95,1.3, border = "brown", lwd = 2)
rect(0.1,-0.15,0.6,0.45, border = "purple", lwd = 2)
rect(-0.95,-0.75,-0.6,0.15, border = "red", lwd = 2)
rect(-0.7,0.45,-0.1,1.6, border = "cyan", lwd = 2)

plot(x, ty = "n", xlab = "", ylab = "", asp = 1, main = "Iter 6")
text(x[hc$order,1], x[hc$order,2], hc$order)
rect(0.5,0.7,0.9,1.1, border = "green", lwd = 2)
rect(0.22,0.67,0.95,1.3, border = "brown", lwd = 2)
rect(0.1,-0.15,0.6,0.45, border = "purple", lwd = 2)
rect(-0.95,-0.75,-0.6,0.15, border = "red", lwd = 2)
rect(-0.7,0.45,-0.1,1.6, border = "cyan", lwd = 2)
rect(0,-0.3,1,1.4, border = "orange", lwd = 2)
```

