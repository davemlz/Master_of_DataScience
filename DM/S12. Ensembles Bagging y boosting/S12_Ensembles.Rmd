---
title: "Ensembles"
output:
  html_document:
    df_print: paged
---

## Bagging

### Random Forest

```{r}
require(randomForest)
```

```{r}
n = nrow(iris)
# train / test partition
indtrain = sample(1:n, round(0.75*n)) # indices for train
indtest = setdiff(1:n, indtrain ) # indices for test
```

```{r}
# RF
rf = randomForest(Species ~., iris , subset = indtrain)
# RF configuration: no. of trees?
# n o. of predictors
# considered at each node?
rf
```

```{r}
# OOB error
plot(rf$err.rate[,1], type = "b",xlab = "no trees",ylab = "OBB error")
grid()
```

```{r}
# prediction for test
pred = predict(rf,iris[indtest,])
# accuracy
sum(diag(table(pred,iris$Species[indtest]))) / length(indtest)
```

```{r}
# comparison with a single tree
library(tree)
t = tree(Species ~.,iris, subset = indtrain)
# prediction for test
pred.t = predict(t,iris[indtest,],type = "class")
# accuracy
sum(diag(table(pred.t,iris$Species[indtest])))/length(indtest)
```

```{r}
urlData = "https://github.com/davemlz/Master_of_DataScience/blob/master/Data_Mining/S12.%20Ensembles%20Bagging%20y%20boosting/meteo.RData?raw=true"
load(url(urlData))
```

```{r}
n = 1000
y = y[1:n]
x = x[1:n,]
```

```{r}
indtrain = sample(1:n, round(0.75*n)) # indices for train
indtest = setdiff(1:n, indtrain ) # indices for test
```

```{r}
occ = y
occ[which(y < 1)] = 0
occ[which(y >= 1)] = 1
```

```{r}
sum(occ)/length(occ)
```

```{r}
df.occ = data.frame(y.occ = as.factor(occ),predictors = x)
```

```{r}
rf = randomForest(y.occ ~ .,df.occ,subset = indtrain)
rf
```

```{r}
plot(rf$err.rate[,1],type = "l",xlab = "no.trees",ylab = "OOB error")
```

```{r}
pred = predict(rf,df.occ[indtest,])
1 - sum(diag(table(pred,df.occ$y.occ[indtest])))/length(indtest)
```

```{r}
ntree = which(rf$err.rate[,1] == min(rf$err.rate[,1]))
```

```{r}
err.oob = c()
for(mtry in 1:20){
  
  rf.mtry = randomForest(y.occ ~ .,df.occ,subset = indtrain,ntree = ntree,mtry = mtry)
  err.oob[mtry] = rf.mtry$err.rate[,1]
  
}

plot(err.oob,type = "b",xlab = "no.predictors",ylab = "OOB error")
```

```{r}
require(MASS)

n = nrow(Boston)

indtrain = sample(1:n, round(0.75*n)) # indices for train
indtest = setdiff(1:n, indtrain ) # indices for test

rf = randomForest(medv ~ .,Boston,subset = indtrain)
rf
```

```{r}
plot(rf$mse,type = "l",xlab = "no.trees",ylab = "OOB error")
```

```{r}
ntree = which(rf$mse == min(rf$mse))
```

```{r}
err.oob = c()
for(mtry in 1:13){
  
  rf.mtry = randomForest(medv ~ .,Boston,subset = indtrain,ntree = ntree,mtry = mtry)
  err.oob[mtry] = rf.mtry$mse[ntree]
  
}

plot(err.oob,type = "b",xlab = "no.predictors",ylab = "OOB error")
```

```{r}
err.test = c()
for(mtry in 1:13){
  
  rf.mtry = randomForest(medv ~ .,Boston,subset = indtrain,ntree = ntree,mtry = mtry)
  pred.mtry = predict(rf.mtry, Boston[indtest,])
  err.test[mtry] = mean((pred.mtry - Boston$medv[indtest])^2)
  
}

plot(err.test,type = "b",xlab = "no.predictors",ylab = "Test error")
```

```{r}
matplot(1:13,cbind(err.oob,err.test),type = "b",pch = 19,lty = 1,col = c("black","red"),ylab = "MSE",xlab = "no.predictors")

legend("topright",c("OOB","Test"),lty = 1,col = c("black","red"))
```

## Boosting

```{r}
require(adabag)
```

```{r}
n = 1000
y = y[1:n]
x = x[1:n,]
```

```{r}
indtrain = sample(1:n, round(0.75*n)) # indices for train
indtest = setdiff(1:n, indtrain ) # indices for test
```

```{r}
occ = y
occ[which(y < 1)] = 0
occ[which(y >= 1)] = 1
```

```{r}
ab = boosting(y.occ ~ .,df.occ[indtrain,],mfinal = 20)
```

```{r}
plot(errorevol(ab,df.occ[indtrain,]))
```

```{r}
plot(ab$trees[[1]])
text(ab$trees[[1]],pretty = FALSE)
```

```{r}
pred.ab = predict(ab,df.occ[indtest,])

1 - sum(diag(table(pred.ab$class,df.occ$y.occ[indtest])))/length(indtest)
```

