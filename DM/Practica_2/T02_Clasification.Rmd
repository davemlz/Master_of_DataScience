---
title: "Tarea 2. Problemas de Clasificación - Reducción de la Dimensión"
output:
  html_document:
    df_print: paged
---

## Minería de Datos (Master en Data Science, UIMP-UC)
### [Profesores: Steven Van Vaerenbergh, Rodrigo G. Manzanas, Joaquín Bedia y Sixto Herrera]
### [Estudiante: David Montero Loaiza]

```{r}
require(caret)
require(ggplot2)
require(AUC)
```

```{r}
set.seed(666)
```

En la presente tarea consideraremos el dataset `meteo.csv`, que podéis descargaros en el GitHub dedicado a este Máster ([meteo.csv](https://github.com/SantanderMetGroup/Master-Data-Science/blob/master/Data_mining/datasets/meteo.csv.)) y que ha sido utilizado en diferentes sesiones prácticas. Dicho dataset contiene en la primera columna el valor de precipitación observado en Lisboa en el periodo 1979-2008 mientras que en las restantes contiene los valores observados de diferentes variables atmosféricas en 40 puntos que cubren aproximadamente la Península Ibérica. Dichas variables serán los `predictores` del modelo mientras que la precipitación será nuestra variable objetivo. En particular los predictores son:

* Altura geopotencial en 500 hPa (columnas 2:41),
* Temperatura del aire en 850 hPa (columnas 42:81), 700 hPa (columnas 82:121) y 500 hPa (columnas 122:161), 
* Temperatura del aire en superficie (columnas 162:201),
* Humedad específica del aire en 850 hPa (columnas 202:241) y 500 hPa (columnas 242:281) y 
* Presión al nivel del mar (columnas 282:321)

```{r}
datasetURL = "https://raw.githubusercontent.com/SantanderMetGroup/Master-Data-Science/master/Data_mining/datasets/meteo.csv"

meteo = read.csv(url(datasetURL))
```

Para establecer el problema de clasificación consideraremos dos umbrales de discretización, 1 mm y 20 mm, que definen la ocurrencia de precipitación (Wet days) y de precipitaciones intensas (Very heavy precipitation days), respectivamente. Puedes consultar más detalles de la definición en la web de [ECA&D](https://www.ecad.eu//indicesextremes/indicesdictionary.php).

```{r}
# QUITAMOS EL ID
meteo = meteo[,-1]
```

```{r}
# WET DAYS
wetdays = meteo
wetdays$y[wetdays$y <= 1] = 0
wetdays$y[wetdays$y != 0] = 1
```

```{r}
# VERY HEAVY PRECIPITATION DAYS
heavydays = meteo
heavydays$y[heavydays$y <= 20] = 0
heavydays$y[heavydays$y != 0] = 1
```

Para el desarrollo de la tarea se permitirá el uso de todo el material incluido en el Moodle de las asignatura así como el desarrollado por el alumno durante la realización de las prácticas.

La entrega consisitirá de un notebook de Jupyter ó un R-MarkDown, junto con el archivo html que éste genera. Ambos ficheros se entregarán a través del Moodle de la asignatura en la tarea correspondiente.

### Punto 1 (3 puntos):

En esta primera parte de la tarea trataremos de ilustrar parte de los problemas reflejados en las sesiones teórico-prácticas planteando diferentes experimentos con el dataset `meteo.csv`.
En primer lugar, considerar el dataset completo incluyendo tanto las variables predictoras como la variable objetivo. Por un lado, calculad las componentes principales con y sin estandarización (`Nota:` consultar la ayuda de la función scale -> `? scale`):

```{r}
# CON ESTANDARIZACION
scaledPCA = prcomp(meteo,scale. = TRUE)
```

```{r}
# SIN ESTANDARIZACION
notscaledPCA = prcomp(meteo,scale. = FALSE)
```

* ¿Cómo contribuye en cada caso la variable objetivo a la primera componente principal?

```{r}
# CON ESTANDARIZACION
scaledPCA$rotation[1,1]
```

```{r}
# SIN ESTANDARIZACION
notscaledPCA$rotation[1,1]
```

Con estandarización la variable objetivo contribuye en un 1.03% a la primera componente principal, mientras que sin estandarización contribuye en un 0.02% a la primera componente principal.

* ¿A qué componente principal contribuye principalmente la variable objetivo? ¿qué porcentaje de varianza se explica hasta dicha componente?

```{r}
# FUNCION PARA CALCULAR LA VARIANZA EXPLICADA
explainedVariance = function(pca){
  
  varpc = pca$sdev^2 / sum(pca$sdev^2)
  
  return(cumsum(varpc))
  
}
```

```{r}
# CON ESTANDARIZACION
sort(abs(scaledPCA$rotation[1,]),decreasing = TRUE)
```

```{r}
explainedVariance(scaledPCA)[23]
```

```{r}
# SIN ESTANDARIZACION
sort(abs(notscaledPCA$rotation[1,]),decreasing = TRUE)
```

```{r}
explainedVariance(notscaledPCA)[74]
```

En el caso en que las variables han sido escaladas, la componente principal a la que más aporta la variable objetivo es la 23 (aporta un 55%), alcanzano un 95.6% de varianza explicada, mientras que en el caso en que las variables no han sido escaladas, la componente principal a la que más aporta la variable objetivo es la 74 (aporta un 70%), alcanzando un 99,9% de varianza explicada.

* En base a los puntos anteriores, ¿puede considerarse en alguno de los casos que la variable objetivo se "eliminaría" del modelo debido a su contribución a cada una de las componentes principales?

Dependerá de la cantidad de componentes principales que utilizaremos en el modelo, si definimos un umbral de varianza explicada del 95%, en ambos casos eliminaríamos la componente principal en la que más aporta la variable objeivo.

* ¿Cómo se distribuye la contribución de las diferentes variables del espacio original a la primera componente principal? En caso de no estandarizar, ¿puede inferirse alguna relación entre el rango de las variables y su contribución a la primera componente principal?

```{r}
# CON ESTANDARIZACION
xlabel = "Contribución"
ylabel = "Frecuencia"
thetitle = "Distribución de la contribución con variables escaladas"
hist(scaledPCA$rotation[,1],breaks = 30,xlab = xlabel,ylab = ylabel,main = thetitle)
```

```{r}
# SIN ESTANDARIZACION
xlabel = "Contribución"
ylabel = "Frecuencia"
thetitle = "Distribución de la contribución con variables sin escalar"
hist(notscaledPCA$rotation[,1],breaks = 30,xlab = xlabel,ylab = ylabel,main = thetitle)
```

En el caso de trabajar con las variables sin escalar, puede observarse, según su distribución, que hay variables que aportan muchísimo más que otras y hay grupos 'separados' que contribuyen de maneras similares y muy diferentes a los demás grupos, esto se debe a la escala de los datos y sus unidades.

Este problema se resuelve en el caso de trabajar con las variables escaladas, ya que, según la distribución, estos grupos ya no están 'separados' y hay una 'similitus' en la contribución de las variables a la componente principal.

`Nota 1:` en principio es posible trabajar con el dataset completo sin seleccionar un subconjunto pero si surge algún problema de memoria podéis considerar únicamente los primeros 10 años (~3650 filas).

`Nota 2:` usad las herramientas gráficas vistas durante la práctica para ilustrar los resultados y las conclusiones obtenidas.

### Punto 2 (3 puntos):

A continuación, consideraremos la serie de precipitación discretizada a partir del valor 1 mm, que es el estándar definido para establecer los días en que ha llovido. El objetivo de este apartado es predecir la ocurrencia de precipitación en Lisboa a partir de los predictores antes definidos. Para ello consideraremos, por un lado, las componentes principales obtenidas estandarizando los datos originales y, por otro lado, el método `KNN`. Dividir la muestra en dos subconjuntos, el primero (20 primeros años) lo utilizaremos para calibrar el modelo y obtener su configuración óptima, mientras que el segundo (10 últimos años) lo utilizaremos como test independiente de cara a comparar diferentes métodos.

```{r}
# CALCULAMOS COMPONENTES PRINCIPALES SIN TENER EN CUENTA LA VARIABLE OBJETIVO
PCA = prcomp(meteo[,-1],scale. = TRUE)
```

```{r}
# DIVIDIR DATOS EN PRIMEROS 20 ANOS DE ENTRENAMIENTO
# ULTIMOS 10 ANOS DE TEST
trainidx = 1:round(2*length(wetdays$y)/3)
ytrain = as.factor(wetdays$y[trainidx])
ytest = as.factor(wetdays$y[-trainidx])
xtrain = PCA$x[trainidx,]
xtest = PCA$x[-trainidx,]
```

* Considerad diferentes umbrales de varianza explicada y el número de PCs asociado (p.e. 40%, 60%, 80% y 90%) y obtend el valor óptimo de `K` en cada caso rastreando valores entre 1 y 15, ¿cómo cambia el valor óptimo? En base a los resultados obtenidos, ¿cuantas PCs considerarías para entrenar el modelo? (`Nota:` ver práctica de `KNN`)

```{r}
# VARIANZA EXPLICADA SEGUN NUMERO DE PCs
expVar = explainedVariance(PCA)
```

```{r}
# UMBRALES DE VARIANZA EXPLICADA
thrs = c(0.4,0.6,0.8,0.9)

# COMPONENTES USADOS POR CADA UMBRAL
comps = sapply(thrs,function(x) sum(expVar < x) + 1)

# DATA FRAME CON UMBRALES Y EL NUMERO DE COMPONENTES ASOCIADOS
umbralComps = data.frame(Umbral = thrs,NoComponentes = comps)
umbralComps
```

```{r}
# CROSS VALIDATION CON 5 FOLDS
ctrl = trainControl(method = "cv",number = 5)
```

```{r}
# FUNCION PARA APLICAR UN LAPPLY
tuningModelKNN = function(th,xtrain,ytrain,ctrl){
  
  pcs = sum(expVar < th) + 1

  xtrainth = xtrain[,1:pcs]
  
  trainData = data.frame(y = ytrain,xtrainth)
  
  names(trainData)[-1] = colnames(xtrain)[1:pcs]
  
  mod = train(y ~ .,
              data = trainData,
              method = "knn",
              trControl = ctrl,
              tuneGrid = expand.grid(k = 1:15))
  
  return(mod)
  
}
```

```{r}
# GENERAR MODELOS PARA CADA VALOR DE UMBRAL
# ESCANEANDO K DE 1 A 15
models = lapply(thrs,tuningModelKNN,xtrain = xtrain,ytrain = ytrain,ctrl = ctrl)
```

```{r}
# ORGANIZAR RESUTADOS EN UN DATAFRAME PARA PLOTEAR EN GGPLOT
dfResults = NULL

for(i in 1:length(thrs)){
  
  df = data.frame(models[[i]]$results,
                  models[[i]]$bestTune,
                  umbralComps[i,],
                  Legend = paste0(umbralComps[i,1],' (N Comp: ',umbralComps[i,2],')'))
  
  dfResults = rbind(dfResults,df)
  
}

dfResults$Legend = as.factor(dfResults$Legend)
```

```{r}
# PLOTEAR ACCURACY X K PARA CADA VALOR DE UMBRAL
ggplot(dfResults,aes(x = k,y = Accuracy,color = Legend)) +
  geom_line() +
  geom_point() +
  theme_light() +
  ggtitle("Precision por número de k") +
  scale_color_brewer(palette = "Set1")
```

```{r}
# K OPTIMOS PARA CADA VALOR DE UMBRAL CON SUS DATOS ASOCIADOS
dfOptimos = dfResults[dfResults$k == dfResults$k.1,]
dfOptimos[order(dfOptimos$Accuracy,decreasing = TRUE),]
```

```{r}
# PLOTEAR MEJOR ACCURACY X NUMERO DE COMPONENTES
ggplot(dfOptimos,aes(x = NoComponentes,y = Accuracy,label = k)) +
  geom_line() +
  geom_point(size = 3) +
  theme_light() +
  geom_label(hjust = 1) +
  labs(title = "Mejor precisión obtenida para cada número de componentes\n(umbrales de varianza explicada)",
       caption = "label: k optimo")
```

```{r}
# MEJORES 10 RESULTADOS ENTRE TODOS LOS MODELOS
head(dfResults[order(dfResults$Accuracy,decreasing = TRUE),],10)
```

El mejor modelo (con el accuracy más alto), lo obtiene aquel que utiliza 14 k y los primeros 10 componentes (explicando hasta un 90% de la varianza). En este caso, es adecuado utilizar los 10 primeros componentes, ya que se ha reducido de manera considerable la dimensionalidad y se está utilizando una cantidad de variables óptima que explica hasta un 90% de la varianza de las variables originales con un alto accuracy (86.2%).

```{r}
# ORGANIZANDO DATOS DE TEST
xtestth = xtest[,1:10]
testData = data.frame(y = ytest,xtestth)
truth1mmKNN = ytest

# MODELO SELECCIONADO
selectedModel = models[[4]]
```

* Para la configuración óptima calibrada con el conjunto de entrenamiento realizad la predicción sobre el conjunto de test y estimad los errores cometidos sobre este conjunto.

```{r}
# PREDICCION SOBRE TEST
pred1mmKNN = predict(selectedModel,newdata = testData)
```

```{r}
# MATRIZ DE CONFUSION SOBRE TEST
cf1mmKNN = confusionMatrix(pred1mmKNN,truth1mmKNN)
cf1mmKNN
```

El accuracy del modelo se encuentra rondando el 85%. Al haber más datos por debaj del umbral (1 mm) que por encima de ese, el modeo trata de ajustarse a esta tendencia y por lo tanto en el conjunto de test se clasifica de mejor manera la no ocurrencia que la ourrencia de wet days.

### Punto 3 (2 puntos):

Repetid el experimento anterior considerando la precipitación discretizada a partir del valor 20 mm, que es el estándar definido para establecer los días en que ha llovido de forma intensa.

```{r}
# DIVIDIR DATOS EN PRIMEROS 20 ANOS DE ENTRENAMIENTO
# ULTIMOS 10 ANOS DE TEST
trainidx = 1:round(2*length(heavydays$y)/3)
ytrain = as.factor(heavydays$y[trainidx])
ytest = as.factor(heavydays$y[-trainidx])
xtrain = PCA$x[trainidx,]
xtest = PCA$x[-trainidx,]
```

```{r}
# GENERAR MODELOS PARA CADA VALOR DE UMBRAL
# ESCANEANDO K DE 1 A 15
models = lapply(thrs,tuningModelKNN,xtrain = xtrain,ytrain = ytrain,ctrl = ctrl)
```

```{r}
# ORGANIZAR RESUTADOS EN UN DATAFRAME PARA PLOTEAR EN GGPLOT
dfResults = NULL

for(i in 1:length(thrs)){
  
  df = data.frame(models[[i]]$results,
                  models[[i]]$bestTune,
                  umbralComps[i,],
                  Legend = paste0(umbralComps[i,1],' (N Comp: ',umbralComps[i,2],')'))
  
  dfResults = rbind(dfResults,df)
  
}

dfResults$Legend = as.factor(dfResults$Legend)
```

```{r}
# PLOTEAR ACCURACY X K PARA CADA VALOR DE UMBRAL
ggplot(dfResults,aes(x = k,y = Accuracy,color = Legend)) +
  geom_line() +
  geom_point() +
  theme_light() +
  ggtitle("Precision por número de k") +
  scale_color_brewer(palette = "Set1")
```

```{r}
# K OPTIMOS PARA CADA VALOR DE UMBRAL CON SUS DATOS ASOCIADOS
dfOptimos = dfResults[dfResults$k == dfResults$k.1,]
dfOptimos[order(dfOptimos$Accuracy,decreasing = TRUE),]
```

```{r}
# PLOTEAR MEJOR ACCURACY X NUMERO DE COMPONENTES
ggplot(dfOptimos,aes(x = NoComponentes,y = Accuracy,label = k)) +
  geom_line() +
  geom_point(size = 3) +
  theme_light() +
  geom_label(hjust = 1) +
  labs(title = "Mejor precisión obtenida para cada número de componentes\n(umbrales de varianza explicada)",
       caption = "label: k optimo")
```

```{r}
# MEJORES 10 RESULTADOS ENTRE TODOS LOS MODELOS
head(dfResults[order(dfResults$Accuracy,decreasing = TRUE),],10)
```

* ¿Cómo cambian los valores óptimos de `K` y de número de PCs?

En este caso el valor óptimo de k fue de 15 para todos los umbrales. El accuracy llegó a un punto constante en donde todos los modelos se mantuvieron en un 97.4%, por lo tanto, sólo utilizando una componente principal (alcanzando hasta un 40% de la varianza explicada de las variables originales), ya se obtenía una alta precisión.

```{r}
# ORGANIZANDO DATOS DE TEST
xtestth = xtest
testData = data.frame(y = ytest,xtestth)
truth20mmKNN = ytest

# MODELO SELECCIONADO
selectedModel = models[[1]]
```

* ¿Cómo cambian los errores sobre el conjunto de test en este caso?

```{r}
# PREDICCION SOBRE TEST
pred20mmKNN = predict(selectedModel,newdata = testData)
```

```{r}
# MATRIZ DE CONFUSION SOBRE TEST
cf20mmKNN = confusionMatrix(pred20mmKNN,truth20mmKNN)
cf20mmKNN
```

El accuracy de este modelo es significativamente alto, pero se debe a que casi la totalidad de los días a clasificar son días de no ocurrencia y el modelo se ajustó a dicha tendencia, pues se observa claramente que no pudo clasificar los días de ocurrencia correctamente.

* En caso de existir diferentes significativas en ambos casos, ¿a qué crees que puede ser debido? ¿La frecuencia en la muestra del evento a predecir puede influir en la calidad de la predicción?

El sobreajuste en este modelo es claro, ya que la no ocurrencia de precipitaciones intensas (< 20 mm) es mucho mayor que la ocurrencia de estos días y por tal motivo el modelo se ajusta (en mayor medida que el modelo de días con precipitaciones) a predecir que un día no tiene una ocurrencia de precipitación intensa a que sí la haya. En la matriz de confusión se oserva cómo el modelo clasifica todos los días como si no hubiera ocurrencia, aunque una pequeña porción sí tenga la ocurrencia de estos días.

### Punto 4 (2 puntos):

Considerando el número de PCs óptimo obtenido en el apartado anterior, predecid el conjunto de test utilizando en este caso la regresión logística como método de clasificación (`Nota:` ver `Práctica Clasificacion Lineal` en el Moodle de la asignatura de Estadística o en la de Minería de Datos).

```{r}
# REGRESION LOGISTICA PARA WET DAYS
trainidx = 1:round(2*length(wetdays$y)/3)
ytrain = as.factor(wetdays$y[trainidx])
ytest = as.factor(wetdays$y[-trainidx])
xtrain = PCA$x[trainidx,]
xtest = PCA$x[-trainidx,]

xtrainth = xtrain[,1:10]
  
trainData = data.frame(y = ytrain,xtrainth)
  
names(trainData)[-1] = colnames(xtrain)[1:10]

mod1mmGLM = train(y ~ .,
              data = trainData,
              method = "glm",
              trControl = ctrl,
              family = "binomial")

truth1mmGLM = ytest
pred1mmGLM = predict(mod1mmGLM,newdata = testData)

# MATRIZ DE CONFUSION SOBRE TEST
cf1mmGLM = confusionMatrix(pred1mmGLM,truth1mmGLM)
cf1mmGLM
```

```{r}
# REGRESION LOGISTICA PARA HEAVY DAYS
trainidx = 1:round(2*length(heavydays$y)/3)
ytrain = as.factor(heavydays$y[trainidx])
ytest = as.factor(heavydays$y[-trainidx])
xtrain = PCA$x[trainidx,]
xtest = PCA$x[-trainidx,]

xtrainth = xtrain[,1]
  
trainData = data.frame(y = ytrain,xtrainth)
  
names(trainData)[-1] = colnames(xtrain)[1]

mod20mmGLM = train(y ~ .,
              data = trainData,
              method = "glm",
              trControl = ctrl,
              family = "binomial")

truth20mmGLM = ytest
pred20mmGLM = predict(mod20mmGLM,newdata = testData)

# MATRIZ DE CONFUSION SOBRE TEST
cf20mmGLM = confusionMatrix(pred20mmGLM,truth20mmGLM)
cf20mmGLM
```

* Comparad los resultados obtenidos utilizando ambos métodos para la predicción de la ocurrencia de precipitación y de precipitación intensa (`Nota:` considerar, por ejemplo, la curva ROC para la comparación), ¿alguno de los métodos se comporta mejor que el otro de forma sistemática?

```{r}
dfAUC = data.frame(Model = c("KNN","Regresión Logística"),
                   WetDays_AUC = c(auc(roc(pred1mmKNN,truth1mmKNN)),auc(roc(pred1mmGLM,truth1mmGLM))),
                   HeavyDays_AUC = c(auc(roc(pred20mmKNN,truth20mmKNN)),auc(roc(pred20mmGLM,truth20mmGLM))))

dfAUC
```

Para el caso de los días con precipitación, el AUC muestra un mayor valor para la Regresión Logística que para el KNN, demostrando una menor probabilidad de obtener falsos positivos y falsos negativos para la Regresión Logística. En el caso de los días con precipitaciones intensas, en ninguno de los casos el modelo tiene una capacidad de separación de clases adecuada, ya que al tener la mayoría de los casos una no ocurrencia de precipitación intensa, ninguno de los modelos pudo adecuarse al hecho de que precipitaciones intensas podían ocurrir. Existe en ese caso los datos demuestran que están desbalanceados y esto afecta claramente la predicción sobre nuevos datos.