{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Kernel Methods\n",
    "\n",
    "Course: Machine Learning II, Data Science Master (Universidad de Cantabria - UIMP).  \n",
    "Lecturer: Steven Van Vaerenbergh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# inline plots\n",
    "%matplotlib inline\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "\n",
    "Kernel PCA is the extension of PCA to the kernel feature space.\n",
    "\n",
    "Typical applications of kernel PCA include:\n",
    "- Nonlinear dimensionality reduction\n",
    "- Nonlinear correlation analysis\n",
    "- Noise reduction\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. Obtener la matriz kernel ${\\bf K}$ con entradas $\\kappa({\\bf x}_i, {\\bf x}_j)$\n",
    "2. Centrar la matriz ${\\bf K}_c = \\left({\\bf I} - {\\bf 1}{\\bf 1}^T\\right) {\\bf K} \\left({\\bf I} -{\\bf 1}{\\bf 1}^T \\right)$\n",
    "3. $[{\\bf V}, \\boldsymbol{\\Lambda}] = {\\rm eig}({\\bf K}_c)$\n",
    "4. $\\boldsymbol{\\alpha}_j = \\lambda_j^{-1/2} {\\bf v}_j$, $j= 1,\\ldots,r$\n",
    "5. for $i=1:n$\n",
    "    - ${\\bf k}_i = \\begin{bmatrix} \\kappa({\\bf x}_i, {\\bf x}_1) & \\ldots &  \\kappa({\\bf x}_i, {\\bf x}_n) \\end{bmatrix}^T$\n",
    "    - ${\\bf y}_i = \\begin{bmatrix} \\boldsymbol{\\alpha}_1^T {\\bf k}_i& \\ldots &  \\boldsymbol{\\alpha}_r^T {\\bf k}_i \\end{bmatrix}^T $\n",
    "\n",
    "## scikit-learn implementation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Kernel PCA for dimensionality reduction\n",
    "\n",
    "## Data sets\n",
    "\n",
    "First, we will generate some synthetic data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two circles data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "# Plot data\n",
    "[X, y] = [X_circles, y_circles]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=\"red\");\n",
    "plt.title(\"Two circles data set\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"U\" data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_data(npbr=100, noise_var=0.01):\n",
    "    # npbr: number of data per branch\n",
    "   \n",
    "    x1a = np.linspace(1,1,npbr) + np.random.normal(0,noise_var,npbr)\n",
    "    x2a = np.linspace(0,2,npbr) + np.random.normal(0,noise_var,npbr)\n",
    "\n",
    "    x1b = np.linspace(-1,-1,npbr) + np.random.normal(0,noise_var,npbr)\n",
    "    x2b = np.linspace(0,2,npbr) + np.random.normal(0,noise_var,npbr)\n",
    "\n",
    "    theta = np.linspace(0, -np.pi, npbr)\n",
    "    x1c = np.cos(theta) + np.random.normal(0,noise_var,npbr)\n",
    "    x2c = np.sin(theta) + np.random.normal(0,noise_var,npbr)\n",
    "\n",
    "    Xa = np.hstack((x1a.reshape(-1,1), x2a.reshape(-1,1)))\n",
    "    Xb = np.hstack((x1b.reshape(-1,1), x2b.reshape(-1,1)))\n",
    "    Xc = np.hstack((x1c.reshape(-1,1), x2c.reshape(-1,1)))\n",
    "\n",
    "    X = np.vstack((Xa,Xb,Xc))\n",
    "\n",
    "    return X\n",
    "\n",
    "n_per_branch = 100 # number of points in each branch\n",
    "noise_var = 0.05 # noise_variance\n",
    "\n",
    "X_u = u_data(npbr=n_per_branch, noise_var=noise_var)\n",
    "\n",
    "# Plot data\n",
    "X = X_u\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=\"red\");\n",
    "plt.title(\"Two circles data set\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCA on circles data\n",
    "\n",
    "Now let us apply KPCA on the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# circles data\n",
    "X = X_circles\n",
    "\n",
    "# Apply KPCA\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, label='data')\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "# grid of test data\n",
    "X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "\n",
    "# projection on the first principal component (in the phi space)\n",
    "Z_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\n",
    "cs = plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "cs.collections[0].set_label('KPCA isolines')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "kpca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gray we plotted the contours of constant projection onto the first principal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 5 eigenvalues: {}'.format(kpca.lambdas_[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the contours of constant projection onto the principal directions 1 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in np.arange(0,6):\n",
    "    plt.subplot(2, 3, i+1, aspect='equal')\n",
    "    plt.title(\"Principal direction %d\"%(i+1))\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=20)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "    X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "    X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "\n",
    "    # projection on the first principal component (in the phi space)\n",
    "    Z_grid = kpca.transform(X_grid)[:, i].reshape(X1.shape)\n",
    "    plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some color that allows to visualize the contour lines better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for i in np.arange(0,6):\n",
    "    plt.subplot(2, 3, i+1, aspect='equal')\n",
    "    plt.title(\"Principal direction %d\"%(i+1))\n",
    "\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "    X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "    X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "\n",
    "    # projection on the first principal component (in the phi space)\n",
    "    Z_grid = kpca.transform(X_grid)[:, i].reshape(X1.shape)\n",
    "    plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "    \n",
    "    plt.pcolormesh(X1, X2, Z_grid, cmap = 'viridis')\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=\"red\", s=20)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a closer look at the retrieved components. We will plot component 1 vs component 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1, 2, 1, aspect='equal')\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.scatter(X[reds, 0], X_circles[reds, 1], c=\"red\", s=20)\n",
    "plt.scatter(X[blues, 0], X_circles[blues, 1], c=\"blue\", s=20)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.subplot(1, 2, 2, aspect='equal')\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\", s=20)\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\", s=20)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCA on U data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_u\n",
    "\n",
    "# Apply KPCA\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=1)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Original space\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"red\", s=20)\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(-2.5, 2.5, 200), np.linspace(-1.5, 2.5, 200))\n",
    "X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "\n",
    "# projection on the first principal component (in the phi space)\n",
    "Z_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\n",
    "plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 5 eigenvalues: {}'.format(kpca.lambdas_[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(-2.5, 2.5, 200), np.linspace(-1.5, 2.5, 200))\n",
    "X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "\n",
    "for i in np.arange(0,6):\n",
    "    plt.subplot(2, 3, i+1, aspect='equal')\n",
    "    plt.title(\"Principal direction %d\"%(i+1))\n",
    "\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "    # projection on the first principal component (in the phi space)\n",
    "    Z_grid = kpca.transform(X_grid)[:, i].reshape(X1.shape)\n",
    "    plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "    plt.pcolormesh(X1, X2, Z_grid, cmap = 'viridis')\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=\"red\", s=20)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a closer look at the retrieved components. We will plot component 1 vs component 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1, 2, 1, aspect='equal')\n",
    "plt.title(\"Original space\")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.subplot(1, 2, 2, aspect='equal')\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], s=20)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: KPCA with different kernels\n",
    "\n",
    "Perform KPCA with different kernels on the \"U\" data set.\n",
    "\n",
    "- Choose between \"rbf\", \"poly\", \"linear\" or a kernel defined by yourself.\n",
    "- Try different parameter settings.\n",
    "\n",
    "#### Unfolding the \"U\" data with KPCA\n",
    "- Can you find a kernel and parameter setting for which the first two KPCA projections (right plot) map the data approximately to a 1D space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the next line\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=1)\n",
    "\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1, 2, 1, aspect='equal')\n",
    "plt.title(\"Original space\")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.subplot(1, 2, 2, aspect='equal')\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], s=20)\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. KPCA for Denoising\n",
    "\n",
    "As a dimensionality reduction technique, KPCA allows to recover the nonlinear manifold that underlies a given data set.\n",
    "The manifold is spanned by a limited set of (kernel) principal directions, and by projecting the data onto this manifold it becomes possible to remove components of the data that contain mostly noise. \n",
    "\n",
    "We will apply KPCA for some simple denoising problem in the following examples. First, we define two data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarter circles data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_circles_data(points=1000, radius=2, noise=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates syntethic data in the shape of two quarter circles, as in\n",
    "    the example from the paper by Mika et al.\n",
    "    Arguments:\n",
    "        points: number of points in the generated dataset.\n",
    "        noise: name of the distribution to be used as additive noise.\n",
    "               Use one of the distribution from numpy.random, see\n",
    "               https://docs.scipy.org/doc/numpy-1.10.0/reference/routines.random.html\n",
    "               Default is 'uniform', with low=-0.5 and high=0.5. Noise is added to\n",
    "               the semi-circle's radious.\n",
    "        args, kwargs: Any arguments you want to pass to the corresponding\n",
    "                      numpy.random sampling function, except size.\n",
    "    Returns:\n",
    "        Arrays with the X and Y coordinates for the new data.\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = 'uniform'\n",
    "        kwargs['low'] = -0.5\n",
    "        kwargs['high'] = 0.5\n",
    "    kwargs['size'] = points // 2\n",
    "    dist = getattr(np.random, noise)\n",
    "\n",
    "    angles = np.linspace(0, np.pi/2, num=points//2)\n",
    "    cos = np.cos(angles)\n",
    "    sin = np.sin(angles)\n",
    "    left_center = -0.5\n",
    "    left_radius = radius + dist(*args, **kwargs)\n",
    "    left_x = -left_radius*cos + left_center\n",
    "    left_y = left_radius*sin\n",
    "    right_center = 0.5\n",
    "    right_radius = radius + dist(*args, **kwargs)\n",
    "    right_x = right_radius*cos[::-1] + right_center\n",
    "    right_y = right_radius*sin[::-1]\n",
    "    \n",
    "    x1 = np.hstack((left_x,right_x))\n",
    "    x2 = np.hstack((left_y,right_y))\n",
    "    X = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_quarter = quarter_circles_data(noise='normal', scale=0.2)\n",
    "\n",
    "# Plot data\n",
    "X = X_quarter\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=\"red\");\n",
    "plt.title(\"Quarter circles data set\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_square(points=1000, length=4, noise=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates syntethic data in the shape of a square.\n",
    "    Arguments:\n",
    "        points: number of points in the generated dataset.\n",
    "        noise: name of the distribution to be used as additive noise.\n",
    "               Use one of the distribution from numpy.random, see\n",
    "               https://docs.scipy.org/doc/numpy-1.10.0/reference/routines.random.html\n",
    "               Default is 'uniform', with low=-0.5 and high=0.5. Noise is added\n",
    "               in the direction orthogonal to the current side.\n",
    "        args, kwargs: Any arguments you want to pass to the corresponding\n",
    "                      numpy.random sampling function, except size.\n",
    "    Returns:\n",
    "        Arrays with the X and Y coordinates for the new data.\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = 'uniform'\n",
    "        kwargs['low'] = -0.5\n",
    "        kwargs['high'] = 0.5\n",
    "    kwargs['size'] = points // 4\n",
    "    dist = getattr(np.random, noise)\n",
    "\n",
    "    real_values = np.linspace(0, length, num=points//4)\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    # Left side\n",
    "    x_values.append(dist(*args, **kwargs))\n",
    "    y_values.append(real_values)\n",
    "    # Right side\n",
    "    x_values.append(dist(*args, **kwargs) + length)\n",
    "    y_values.append(real_values)\n",
    "    # Top side\n",
    "    x_values.append(real_values)\n",
    "    y_values.append(dist(*args, **kwargs) + length)\n",
    "    # Bottom side\n",
    "    x_values.append(real_values)\n",
    "    y_values.append(dist(*args, **kwargs))\n",
    "    \n",
    "    X = np.hstack((np.array(x_values).reshape(-1,1),np.array(y_values).reshape(-1,1)))\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_square = get_square(noise='normal', scale=0.2)\n",
    "\n",
    "# Plot data\n",
    "X = X_square\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=\"red\");\n",
    "plt.title(\"Square data set\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply KPCA, project the data onto a limited number of principal directions in feature space.\n",
    "And, finally, we apply the inverse transform to obtain pre-images of the data in the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_quarter\n",
    "\n",
    "# apply KPCA and maintain only a limited number of components\n",
    "kpca = KernelPCA(n_components=5, kernel=\"rbf\", fit_inverse_transform=True, gamma=1)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# obtain the pre-images in the input space\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:,0],X[:,1],color='blue')\n",
    "plt.scatter(X_back[:,0],X_back[:,1],color='red')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the red pre-images represent denoised versions of the original data.\n",
    "\n",
    "We now repeat this experiment with a noisy data set shaped as a square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_square\n",
    "\n",
    "# apply KPCA and maintain only a limited number of components\n",
    "kpca = KernelPCA(n_components=5, kernel=\"rbf\", fit_inverse_transform=True, gamma=1)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# obtain the pre-images in the input space\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:,0],X[:,1],color='blue')\n",
    "plt.scatter(X_back[:,0],X_back[:,1],color='red')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Kernel-based clustering techniques\n",
    "\n",
    "- **Kernel K-means** is a direct extension of K-means in the kernel feature space\n",
    "- **Spectral clustering** is performed by applying KPCA followed by K-means on the principal components.\n",
    "\n",
    "The scikit-learn implementation of spectral clustering can be found here: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html\n",
    "\n",
    "Let us apply kernel K-means on the 2 circles data set.\n",
    "Note that kernel K-means is not included in scikit-learn.\n",
    "KernelKMeans we will be using a scikit-learn compatible 3rd party implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernel_kmeans import KernelKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Kernel K-means parameters\n",
    "\n",
    "In the next code block, set an appropriate kernel parameter for kernel K-means such that it finds the clusters correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkm = KernelKMeans(n_clusters=2, random_state=0, kernel='rbf', gamma=.1)\n",
    "\n",
    "X = X_circles\n",
    "y_pred = kkm.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='bwr', edgecolors='none')\n",
    "plt.title('Kernel K-means')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Spectral clustering parameters\n",
    "\n",
    "In the next code block, set an appropriate kernel parameter for spectral clustering such that it finds the clusters correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack',\n",
    "        affinity=\"rbf\",gamma=.1,n_jobs=2,random_state=0)\n",
    "\n",
    "X = X_circles\n",
    "y_pred = spc.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='bwr', edgecolors='none')\n",
    "plt.title('Spectral clustering')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two moons data set\n",
    "\n",
    "Let us try both techniques on another data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "X_moons, y_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "# Plot data\n",
    "[X, y] = [X_moons, y_moons]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c=\"red\");\n",
    "plt.title(\"Two moons data set\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkm = KernelKMeans(n_clusters=2,random_state=0, kernel='rbf', gamma=1)\n",
    "\n",
    "spc = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack',\n",
    "        affinity=\"rbf\",gamma=15,n_jobs=2,random_state=0)\n",
    "\n",
    "X = X_moons\n",
    "\n",
    "y_pred1 = kkm.fit_predict(X)\n",
    "y_pred2 = spc.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred1, cmap='bwr', edgecolors='none')\n",
    "plt.title('Kernel K-means')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred2, cmap='bwr', edgecolors='none')\n",
    "plt.title('Spectral clustering')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find a suitable parameter for kernel K-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two spirals data set\n",
    "\n",
    "Now we apply spectral clustering on the \"two spirals\" data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_spirals_data(n_points, noise=.5):\n",
    "    \"\"\"\n",
    "     Returns the two spirals dataset.\n",
    "    \"\"\"\n",
    "    theta = .5+np.sqrt(np.random.rand(n_points,1)) * 720 * (2*np.pi)/360\n",
    "    d1x = -np.cos(theta)*theta + np.random.rand(n_points,1) * noise\n",
    "    d1y = np.sin(theta)*theta + np.random.rand(n_points,1) * noise\n",
    "    \n",
    "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
    "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
    "\n",
    "n_points = 1500\n",
    "X_spirals, y_spirals = two_spirals_data(n_points = n_points)\n",
    "\n",
    "# Plot data\n",
    "[X, y] = [X_spirals, y_spirals]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, c=\"red\");\n",
    "plt.title(\"Two spirals data set\")\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "spectral = cluster.SpectralClustering(n_clusters=2, eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\",n_jobs=2)\n",
    "\n",
    "X = X_spirals\n",
    "start = time()\n",
    "y_pred1 = spectral.fit_predict(X)\n",
    "elapsed = time() - start\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred1, cmap='bwr', edgecolors='none')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Elapsed time: %.2f s.\"%elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concentric circles data set\n",
    "\n",
    "Finally, we create a set of concentric circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concentric_circles_data(n_circle_pairs=2,n_samples_unit=300):\n",
    "\n",
    "    # initialize X and y as empty\n",
    "    X = np.empty((0,2), int)\n",
    "    y = np.empty((0,), int)\n",
    "\n",
    "    for i in np.arange(0,n_circle_pairs):\n",
    "        Xi,yi = datasets.make_circles(n_samples=np.round(n_samples_unit*(1+i)),\n",
    "                                      factor=(.5+i)/(1+i), noise=.05/(1+i))\n",
    "\n",
    "        X = np.concatenate((X,Xi*(1+i)),axis=0)\n",
    "        y = np.concatenate((y,yi+2*i),axis=0)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "n_circle_pairs = 3\n",
    "n_samples_unit = 300\n",
    "\n",
    "X_circles, y_circles = concentric_circles_data(n_circle_pairs,n_samples_unit)\n",
    "\n",
    "X, y = X_circles, y_circles\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:,0],X[:,1],cmap='viridis')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print('Number of points: %d'%len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we apply spectral clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "spc = SpectralClustering(n_clusters=n_circle_pairs*2, n_init=10,\n",
    "                         gamma=100.0, affinity='rbf', n_jobs=2)\n",
    "\n",
    "start = time()\n",
    "y_pred = spc.fit_predict(X)\n",
    "elapsed = time() - start\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(X[:,0],X[:,1],c=y_pred,cmap='viridis')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"Elapsed time: %.2f s.\"%elapsed)\n",
    "\n",
    "spc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Moving targets\n",
    "\n",
    "In this exercise we will apply clustering to perform target tracking of two objects (\"sources\").\n",
    "\n",
    "Consider two sources, one located at [1,0] and another located at [-1,0].\n",
    "At each time instant, for `t=0,1,2,\\dots`, we observe the location of one of the two source.\n",
    "However, we do not know beforehand which source is active at each time instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_points = 200\n",
    "noise_scale = 0.2\n",
    "\n",
    "y = np.random.randint(2,size=n_points)\n",
    "\n",
    "x1 = np.ones(n_points) + np.random.normal(scale=noise_scale,size=n_points)\n",
    "x2 = np.zeros(n_points) + np.random.normal(scale=noise_scale,size=n_points)\n",
    "\n",
    "X = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\n",
    "X[y==1] *= -1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap='bwr')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine the sources are moving in a circular fashion in time.\n",
    "We simulate this by setting the `final_angle` parameter to `2*np.pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "np.random.seed(0)\n",
    "\n",
    "n_points = 200\n",
    "noise_scale = 0.2\n",
    "\n",
    "#final_angle = np.pi/2\n",
    "final_angle = 2*np.pi\n",
    "\n",
    "y = np.random.randint(2,size=n_points)\n",
    "\n",
    "theta = np.linspace(0,final_angle,n_points)\n",
    "x1 = np.cos(theta) + np.random.normal(scale=noise_scale,size=n_points)\n",
    "x2 = np.sin(theta) + np.random.normal(scale=noise_scale,size=n_points)\n",
    "\n",
    "X = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\n",
    "X[y==1] *= -1\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap='bwr')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only access to the observations X, how could we cluster these points into groups that correspond to the two sources?\n",
    "\n",
    "One option is to take into a account the temporal dimension.\n",
    "In this exercise, you will do the following:\n",
    "1. Add a new dimension with a \"temporal index\" to the data, i.e. construct an \"extended\" data set `X_ext = [X0, X1, t]`. The temporal index `t` should be a scaled version of the data indices.\n",
    "2. Then, run spectral clustering on the extended data. Tune the parameters of spectral clustering such that it obtains a clustering error of 0 data points.\n",
    "3. Plot the result of the clustering in 3D using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate the extended data\n",
    "\n",
    "# generate data\n",
    "np.random.seed(0)\n",
    "\n",
    "n_points = 200\n",
    "noise_scale = 0.2\n",
    "\n",
    "final_angle = 2*np.pi\n",
    "\n",
    "y = np.random.randint(2,size=n_points)\n",
    "\n",
    "theta = np.linspace(0,final_angle,n_points)\n",
    "x1 = np.cos(theta) + np.random.normal(scale=noise_scale,size=n_points)\n",
    "x2 = np.sin(theta) + np.random.normal(scale=noise_scale,size=n_points)\n",
    "\n",
    "X = np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\n",
    "X[y==1] *= -1\n",
    "\n",
    "### change the next line: ###\n",
    "Z = np.zeros(n_points)\n",
    "\n",
    "X_ext = np.hstack((X,Z.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform spectral clustering\n",
    "\n",
    "### Replace the parameters to tune spectral clustering ###\n",
    "spc = cluster.SpectralClustering(gamma=.1,\n",
    "        n_clusters=2, eigen_solver='arpack',\n",
    "        affinity=\"rbf\",n_jobs=2)\n",
    "\n",
    "# spectral clustering on the extended data set\n",
    "y_pred = spc.fit_predict(X_ext)\n",
    "\n",
    "errors_a = (y != y_pred).sum()\n",
    "errors_b = (y != 1-y_pred).sum()\n",
    "\n",
    "print('Misclustered samples: %d' % np.minimum(errors_a,errors_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install plotly\n",
    "\"\"\"\n",
    "\n",
    "# load Plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# initiate the Plotly Notebook mode\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Plot the data using Plotly\n",
    "\n",
    "def plot_3D(X, y):\n",
    "    \n",
    "    # retrieve indices of zeroes and ones\n",
    "    idx0 = y==0\n",
    "    idx1 = y==1\n",
    "    \n",
    "    # define data set 1 plot options\n",
    "    trace0 = go.Scatter3d(x=X[idx0,0], y=X[idx0,1], z=X[idx0,2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='red', opacity=0.8)\n",
    "    )\n",
    "\n",
    "    # define data set 2 plot options\n",
    "    trace1 = go.Scatter3d(x=X[idx1,0], y=X[idx1,1], z=X[idx1,2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color='blue', opacity=0.8)\n",
    "    )\n",
    "\n",
    "    # set aspect ratio\n",
    "    scene = dict(aspectmode=\"manual\", aspectratio=dict(x=1, y=1, z=1))\n",
    "\n",
    "    # define figure properties\n",
    "    layout = go.Layout(\n",
    "        scene=scene,\n",
    "        height=600,\n",
    "        width=900\n",
    "    )\n",
    "\n",
    "    # produce the plot\n",
    "    fig = go.Figure(data=[trace0,trace1],layout=layout)\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3D(X_ext,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Spectral clustering for image segmentation\n",
    "\n",
    "Finally, we will analyze a simple application of spectral clustering for image segmentation.\n",
    "\n",
    "In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.\n",
    "\n",
    "In these settings, the Spectral clustering approach solves the problem know as \"normalized graph cuts\": the image is seen as a graph of connected voxels, and the spectral clustering algorithm amounts to choosing graph cuts defining regions while minimizing the ratio of the gradient along the cut, and the volume of the region.\n",
    "\n",
    "As the algorithm tries to balance the volume (i.e. balance the region sizes), if we take circles with different sizes, the segmentation fails.\n",
    "\n",
    "In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the spectral clustering on a graph that is only weakly informed by the gradient.\n",
    "This is close to performing a Voronoi partition of the graph.\n",
    "\n",
    "In addition, we use the mask of the objects to restrict the graph to the outline of the objects.\n",
    "In this example, we are interested in separating the objects one from the other, and not from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n",
    "#           Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# generate the data for the image\n",
    "\n",
    "l = 100\n",
    "x, y = np.indices((l, l))\n",
    "\n",
    "center1 = (28, 24)\n",
    "center2 = (40, 50)\n",
    "center3 = (67, 58)\n",
    "center4 = (24, 70)\n",
    "\n",
    "radius1, radius2, radius3, radius4 = 16, 14, 15, 14\n",
    "\n",
    "circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2\n",
    "circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2\n",
    "circle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3 ** 2\n",
    "circle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4 ** 2\n",
    "\n",
    "# 4 circles\n",
    "img = circle1 + circle2 + circle3 + circle4\n",
    "mask = img.astype(bool)\n",
    "img = img.astype(float)\n",
    "\n",
    "# add noise\n",
    "img += 1 + 0.2 * np.random.randn(*img.shape)\n",
    "\n",
    "plt.matshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from sklearn.cluster import spectral_clustering\n",
    "\n",
    "# Convert the image into a graph with the value of the gradient on the\n",
    "# edges.\n",
    "graph = image.img_to_graph(img, mask=mask)\n",
    "\n",
    "# Take a decreasing function of the gradient: we take it weakly\n",
    "# dependent from the gradient the segmentation is close to a voronoi\n",
    "graph.data = np.exp(-graph.data / graph.data.std())\n",
    "\n",
    "# Force the solver to be arpack, since amg is numerically\n",
    "# unstable on this example\n",
    "labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')\n",
    "label_im = -np.ones(mask.shape)\n",
    "label_im[mask] = labels\n",
    "\n",
    "plt.matshow(label_im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional reading\n",
    "\n",
    "\"Comparing different clustering algorithms on toy datasets\": https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
